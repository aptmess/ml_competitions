{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"–ö–æ–ø–∏—è 02. LoaderJson.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KDwqlJw0cn7","executionInfo":{"status":"ok","timestamp":1628327609937,"user_tz":-180,"elapsed":89586,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"8898bda0-97e0-4049-d535-2eac36d2e69d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5m4OpF7PEbo","executionInfo":{"status":"ok","timestamp":1628327677642,"user_tz":-180,"elapsed":282,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"7be91924-74d9-4d13-9ecf-744f887f57ce"},"source":["cd /content/drive/MyDrive/digital_breakthrough/task_3"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sbiVOluT5WT","executionInfo":{"status":"ok","timestamp":1628327682531,"user_tz":-180,"elapsed":4605,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"fe6d4dc3-da4c-4793-85b2-4071d31b1847"},"source":["!pip install -r requirements.txt"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: albumentations==0.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.5.2)\n","Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.5.0)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n","Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.9.0+cu102)\n","Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.1.5)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.10.0+cu102)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (4.1.2.30)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (5.4.1)\n","Requirement already satisfied: tqdm==4.56.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (4.56.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.16.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (3.2.2)\n","Requirement already satisfied: python-json-logger>=0.1.11 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (2.0.2)\n","Requirement already satisfied: jupyterlab in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (3.1.4)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.11.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (4.9.1)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (2.1.0)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2->-r requirements.txt (line 1)) (0.4.0)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2->-r requirements.txt (line 1)) (4.5.3.56)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (0.12.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (3.17.3)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (1.6.3)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (1.1.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (3.1.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (0.4.0)\n","Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (2.5.0)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (2.5.0)\n","Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (1.34.1)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (3.3.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (1.12.1)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (0.2.0)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (2.5.0.dev2021032900)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (0.36.2)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (1.1.2)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->-r requirements.txt (line 2)) (3.7.4.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 5)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 5)) (2.8.1)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.2->-r requirements.txt (line 6)) (7.1.2)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 10)) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 10)) (2.5.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 10)) (2.4.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 13)) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 13)) (2.4.7)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0->-r requirements.txt (line 2)) (1.5.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2->-r requirements.txt (line 1)) (1.7.1)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 10)) (4.4.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (1.8.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (0.4.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (1.32.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (57.2.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (4.6.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (3.1.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 11)) (1.0.1)\n","Requirement already satisfied: nbclassic~=0.2 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (0.3.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (5.5.0)\n","Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (6.1)\n","Requirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (2.11.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (4.7.1)\n","Requirement already satisfied: jupyter-server~=1.4 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (1.10.2)\n","Requirement already satisfied: jupyterlab-server~=2.3 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (2.6.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 15)) (21.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 15)) (2.0.1)\n","Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (5.0.5)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.10.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.2.0)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.11.0)\n","Requirement already satisfied: jupyter-client>=6.1.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (6.1.12)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (1.1.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (5.6.1)\n","Requirement already satisfied: requests-unixsocket in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.2.0)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (3.3.0)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (1.7.1)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (5.1.3)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (22.1.0)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (20.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (1.2.0)\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 15)) (3.2.0)\n","Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 15)) (2.9.1)\n","Requirement already satisfied: json5 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 15)) (0.9.6)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 15)) (21.2.0)\n","Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 15)) (0.18.0)\n","Requirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from nbclassic~=0.2->jupyterlab->-r requirements.txt (line 15)) (5.3.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook<7->nbclassic~=0.2->jupyterlab->-r requirements.txt (line 15)) (4.10.1)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 19)) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 19)) (0.10.3)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 19)) (0.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 19)) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 19)) (0.0.45)\n","Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from omegaconf->-r requirements.txt (line 20)) (4.8)\n","Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (2.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->-r requirements.txt (line 2)) (3.5.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 15)) (0.8.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 15)) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 15)) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 15)) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 15)) (2.6.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyterlab->-r requirements.txt (line 15)) (0.2.5)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (3.3.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (1.4.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.7.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 15)) (0.5.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 19)) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwHpMXnKnxJx","executionInfo":{"status":"ok","timestamp":1628327755379,"user_tz":-180,"elapsed":231,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"cf57b043-068a-48ca-dca5-284619443f5b"},"source":["cd /content/drive/MyDrive/digital_breakthrough/task_3"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IeqRmx6SRUGQ"},"source":["from definitions import DATA_PATH\n","import numpy as np\n","import pandas as pd\n","train = pd.read_csv(DATA_PATH / 'train.csv')\n","labels = np.sort(train.typology.unique())\n","typology_to_label = dict(zip(labels, range(len(labels))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0zYiJ7xoKnL"},"source":["best_load = [pd.read_csv(DATA_PATH / f'load_{x}') for x in typology_to_label.keys()]\n","\n","full = pd.concat(best_load, axis=0)\n","\n","fe = full[full.name != -1]\n","fe = fe[fe.typology != -1]\n","df = pd.DataFrame()\n","df[['guid', 'description', 'typology', 'url']] = fe[['guid', 'name', 'typology', 'url']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFOJIyHoRc3o"},"source":["df.to_csv('./data/train_full_load.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJjOyiC0TyBi","executionInfo":{"status":"ok","timestamp":1628327785359,"user_tz":-180,"elapsed":7937,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}}},"source":["from src.train import train_description_model"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nhW8lol3T3qn","executionInfo":{"status":"error","timestamp":1628257604852,"user_tz":-180,"elapsed":2672373,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"52f09735-94a4-46fa-a119-7af661b57b10"},"source":["train_description_model(\"main_exp_v1.yaml\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                               –ø—Ä–æ—á–∏–µ\n","12                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","13                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","14                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/train_data.csv 15\n"],"name":"stdout"},{"output_type":"stream","text":["Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1014250\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 158480\n"],"name":"stderr"},{"output_type":"stream","text":["<transformers.data.datasets.language_modeling.LineByLineTextDataset object at 0x7feab2b21bd0>\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='150145' max='158480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [150145/158480 13:29:47 < 44:57, 3.09 it/s, Epoch 4.74/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.323100</td>\n","      <td>7.699655</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.628400</td>\n","      <td>7.441742</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.352600</td>\n","      <td>7.258137</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.166200</td>\n","      <td>7.083109</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>7.049500</td>\n","      <td>6.920391</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>6.919800</td>\n","      <td>6.806240</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>6.804700</td>\n","      <td>6.691088</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>6.693900</td>\n","      <td>6.530052</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.572300</td>\n","      <td>6.454288</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.476700</td>\n","      <td>6.353993</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.359600</td>\n","      <td>6.252115</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.291700</td>\n","      <td>6.134168</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.201500</td>\n","      <td>6.071229</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.128000</td>\n","      <td>5.996326</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>6.012000</td>\n","      <td>5.894029</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>5.925500</td>\n","      <td>5.849529</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>5.852700</td>\n","      <td>5.765969</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>5.837900</td>\n","      <td>5.685366</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.743300</td>\n","      <td>5.638020</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.731900</td>\n","      <td>5.596389</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.676700</td>\n","      <td>5.530405</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.578600</td>\n","      <td>5.479818</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.526600</td>\n","      <td>5.422380</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.476300</td>\n","      <td>5.370522</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.460800</td>\n","      <td>5.336691</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.416100</td>\n","      <td>5.289468</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.336200</td>\n","      <td>5.257341</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.314200</td>\n","      <td>5.214374</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.291700</td>\n","      <td>5.161703</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.237200</td>\n","      <td>5.138886</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.241100</td>\n","      <td>5.075625</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.143900</td>\n","      <td>5.044663</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>5.102500</td>\n","      <td>5.036678</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>5.056000</td>\n","      <td>5.003049</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>4.988800</td>\n","      <td>4.970918</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>5.016500</td>\n","      <td>4.914900</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>5.054500</td>\n","      <td>4.900282</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>4.980000</td>\n","      <td>4.864625</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>4.934500</td>\n","      <td>4.828434</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>4.914300</td>\n","      <td>4.807549</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>4.885300</td>\n","      <td>4.774049</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>4.867200</td>\n","      <td>4.742507</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>4.848000</td>\n","      <td>4.746023</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>4.784700</td>\n","      <td>4.699873</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>4.725700</td>\n","      <td>4.672996</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>4.770700</td>\n","      <td>4.658352</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>4.734000</td>\n","      <td>4.626236</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>4.727700</td>\n","      <td>4.616520</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>4.686500</td>\n","      <td>4.577704</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>4.630900</td>\n","      <td>4.596302</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>4.665500</td>\n","      <td>4.559931</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>4.611600</td>\n","      <td>4.542715</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>4.608400</td>\n","      <td>4.498367</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>4.562500</td>\n","      <td>4.502635</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>4.561300</td>\n","      <td>4.462436</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>4.546100</td>\n","      <td>4.478402</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>4.536200</td>\n","      <td>4.455986</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>4.503500</td>\n","      <td>4.403316</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>4.499500</td>\n","      <td>4.393417</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>4.493600</td>\n","      <td>4.389234</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>4.457300</td>\n","      <td>4.362535</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>4.470000</td>\n","      <td>4.360286</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>4.456200</td>\n","      <td>4.353082</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>4.438400</td>\n","      <td>4.341885</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>4.363900</td>\n","      <td>4.307459</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>4.325700</td>\n","      <td>4.307482</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>4.363100</td>\n","      <td>4.277255</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>4.350000</td>\n","      <td>4.260219</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>4.304200</td>\n","      <td>4.247015</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>4.251900</td>\n","      <td>4.255249</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>4.301600</td>\n","      <td>4.247949</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>4.306400</td>\n","      <td>4.214637</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>4.246800</td>\n","      <td>4.192334</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>4.222000</td>\n","      <td>4.180785</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>4.254000</td>\n","      <td>4.183067</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>4.214000</td>\n","      <td>4.185728</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>4.216200</td>\n","      <td>4.161015</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>4.241200</td>\n","      <td>4.166919</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>4.192300</td>\n","      <td>4.136303</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>4.211300</td>\n","      <td>4.141234</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>4.163200</td>\n","      <td>4.110280</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>4.194400</td>\n","      <td>4.100849</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>4.126100</td>\n","      <td>4.113945</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>4.184600</td>\n","      <td>4.072514</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>4.111200</td>\n","      <td>4.076514</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>4.132300</td>\n","      <td>4.083775</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>4.102100</td>\n","      <td>4.077248</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>4.109900</td>\n","      <td>4.052927</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>4.100000</td>\n","      <td>4.036159</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>4.050200</td>\n","      <td>4.009013</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>4.020900</td>\n","      <td>4.013702</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>4.063200</td>\n","      <td>4.008592</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>4.093600</td>\n","      <td>4.011169</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>3.999300</td>\n","      <td>3.987392</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>4.077100</td>\n","      <td>3.961033</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>3.999800</td>\n","      <td>3.963388</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>4.003700</td>\n","      <td>3.957594</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>3.998500</td>\n","      <td>3.958097</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>4.008700</td>\n","      <td>3.948853</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>4.016700</td>\n","      <td>3.935455</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>3.998100</td>\n","      <td>3.920242</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>3.976000</td>\n","      <td>3.921383</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>4.010000</td>\n","      <td>3.892124</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>3.951300</td>\n","      <td>3.891581</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>3.999000</td>\n","      <td>3.897561</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>3.958200</td>\n","      <td>3.905873</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>3.943500</td>\n","      <td>3.866236</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>4.005800</td>\n","      <td>3.874210</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>3.932100</td>\n","      <td>3.871142</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>3.900400</td>\n","      <td>3.878453</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>3.923900</td>\n","      <td>3.848500</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>3.923600</td>\n","      <td>3.846391</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>3.878600</td>\n","      <td>3.838409</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>3.910100</td>\n","      <td>3.840129</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>3.881700</td>\n","      <td>3.825138</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>3.909800</td>\n","      <td>3.829822</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>3.892400</td>\n","      <td>3.801601</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>3.916000</td>\n","      <td>3.811776</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>3.847600</td>\n","      <td>3.811334</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>3.833700</td>\n","      <td>3.804014</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>3.855700</td>\n","      <td>3.809561</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>3.840300</td>\n","      <td>3.791081</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>3.839200</td>\n","      <td>3.755806</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>3.828300</td>\n","      <td>3.752411</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>3.851400</td>\n","      <td>3.777727</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>3.838000</td>\n","      <td>3.767160</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>3.782900</td>\n","      <td>3.752557</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>3.765500</td>\n","      <td>3.729413</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>3.779300</td>\n","      <td>3.743454</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>3.755500</td>\n","      <td>3.752195</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>3.744500</td>\n","      <td>3.731699</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>3.782600</td>\n","      <td>3.712841</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>3.752000</td>\n","      <td>3.727095</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>3.712100</td>\n","      <td>3.716960</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>3.720500</td>\n","      <td>3.713384</td>\n","    </tr>\n","    <tr>\n","      <td>68000</td>\n","      <td>3.700700</td>\n","      <td>3.710413</td>\n","    </tr>\n","    <tr>\n","      <td>68500</td>\n","      <td>3.739700</td>\n","      <td>3.689943</td>\n","    </tr>\n","    <tr>\n","      <td>69000</td>\n","      <td>3.735200</td>\n","      <td>3.691814</td>\n","    </tr>\n","    <tr>\n","      <td>69500</td>\n","      <td>3.751900</td>\n","      <td>3.692579</td>\n","    </tr>\n","    <tr>\n","      <td>70000</td>\n","      <td>3.713000</td>\n","      <td>3.668799</td>\n","    </tr>\n","    <tr>\n","      <td>70500</td>\n","      <td>3.691500</td>\n","      <td>3.699093</td>\n","    </tr>\n","    <tr>\n","      <td>71000</td>\n","      <td>3.707700</td>\n","      <td>3.679486</td>\n","    </tr>\n","    <tr>\n","      <td>71500</td>\n","      <td>3.681900</td>\n","      <td>3.670473</td>\n","    </tr>\n","    <tr>\n","      <td>72000</td>\n","      <td>3.719400</td>\n","      <td>3.671936</td>\n","    </tr>\n","    <tr>\n","      <td>72500</td>\n","      <td>3.729200</td>\n","      <td>3.645765</td>\n","    </tr>\n","    <tr>\n","      <td>73000</td>\n","      <td>3.652400</td>\n","      <td>3.659861</td>\n","    </tr>\n","    <tr>\n","      <td>73500</td>\n","      <td>3.680400</td>\n","      <td>3.651311</td>\n","    </tr>\n","    <tr>\n","      <td>74000</td>\n","      <td>3.694700</td>\n","      <td>3.631735</td>\n","    </tr>\n","    <tr>\n","      <td>74500</td>\n","      <td>3.698600</td>\n","      <td>3.632925</td>\n","    </tr>\n","    <tr>\n","      <td>75000</td>\n","      <td>3.654300</td>\n","      <td>3.625872</td>\n","    </tr>\n","    <tr>\n","      <td>75500</td>\n","      <td>3.718200</td>\n","      <td>3.638897</td>\n","    </tr>\n","    <tr>\n","      <td>76000</td>\n","      <td>3.707900</td>\n","      <td>3.627610</td>\n","    </tr>\n","    <tr>\n","      <td>76500</td>\n","      <td>3.646700</td>\n","      <td>3.612916</td>\n","    </tr>\n","    <tr>\n","      <td>77000</td>\n","      <td>3.680000</td>\n","      <td>3.614781</td>\n","    </tr>\n","    <tr>\n","      <td>77500</td>\n","      <td>3.668600</td>\n","      <td>3.622719</td>\n","    </tr>\n","    <tr>\n","      <td>78000</td>\n","      <td>3.657300</td>\n","      <td>3.615793</td>\n","    </tr>\n","    <tr>\n","      <td>78500</td>\n","      <td>3.588800</td>\n","      <td>3.602563</td>\n","    </tr>\n","    <tr>\n","      <td>79000</td>\n","      <td>3.666400</td>\n","      <td>3.594930</td>\n","    </tr>\n","    <tr>\n","      <td>79500</td>\n","      <td>3.627600</td>\n","      <td>3.594881</td>\n","    </tr>\n","    <tr>\n","      <td>80000</td>\n","      <td>3.595200</td>\n","      <td>3.591403</td>\n","    </tr>\n","    <tr>\n","      <td>80500</td>\n","      <td>3.668500</td>\n","      <td>3.579723</td>\n","    </tr>\n","    <tr>\n","      <td>81000</td>\n","      <td>3.691500</td>\n","      <td>3.578439</td>\n","    </tr>\n","    <tr>\n","      <td>81500</td>\n","      <td>3.648800</td>\n","      <td>3.595245</td>\n","    </tr>\n","    <tr>\n","      <td>82000</td>\n","      <td>3.643300</td>\n","      <td>3.572337</td>\n","    </tr>\n","    <tr>\n","      <td>82500</td>\n","      <td>3.606400</td>\n","      <td>3.562537</td>\n","    </tr>\n","    <tr>\n","      <td>83000</td>\n","      <td>3.594700</td>\n","      <td>3.564169</td>\n","    </tr>\n","    <tr>\n","      <td>83500</td>\n","      <td>3.608000</td>\n","      <td>3.563971</td>\n","    </tr>\n","    <tr>\n","      <td>84000</td>\n","      <td>3.620600</td>\n","      <td>3.573940</td>\n","    </tr>\n","    <tr>\n","      <td>84500</td>\n","      <td>3.628400</td>\n","      <td>3.560497</td>\n","    </tr>\n","    <tr>\n","      <td>85000</td>\n","      <td>3.615600</td>\n","      <td>3.574631</td>\n","    </tr>\n","    <tr>\n","      <td>85500</td>\n","      <td>3.595700</td>\n","      <td>3.553571</td>\n","    </tr>\n","    <tr>\n","      <td>86000</td>\n","      <td>3.619800</td>\n","      <td>3.553386</td>\n","    </tr>\n","    <tr>\n","      <td>86500</td>\n","      <td>3.586500</td>\n","      <td>3.533318</td>\n","    </tr>\n","    <tr>\n","      <td>87000</td>\n","      <td>3.555900</td>\n","      <td>3.535371</td>\n","    </tr>\n","    <tr>\n","      <td>87500</td>\n","      <td>3.626800</td>\n","      <td>3.541096</td>\n","    </tr>\n","    <tr>\n","      <td>88000</td>\n","      <td>3.551600</td>\n","      <td>3.529006</td>\n","    </tr>\n","    <tr>\n","      <td>88500</td>\n","      <td>3.511200</td>\n","      <td>3.540694</td>\n","    </tr>\n","    <tr>\n","      <td>89000</td>\n","      <td>3.571600</td>\n","      <td>3.534573</td>\n","    </tr>\n","    <tr>\n","      <td>89500</td>\n","      <td>3.550600</td>\n","      <td>3.508835</td>\n","    </tr>\n","    <tr>\n","      <td>90000</td>\n","      <td>3.579600</td>\n","      <td>3.509177</td>\n","    </tr>\n","    <tr>\n","      <td>90500</td>\n","      <td>3.593200</td>\n","      <td>3.515264</td>\n","    </tr>\n","    <tr>\n","      <td>91000</td>\n","      <td>3.519200</td>\n","      <td>3.525867</td>\n","    </tr>\n","    <tr>\n","      <td>91500</td>\n","      <td>3.544400</td>\n","      <td>3.503102</td>\n","    </tr>\n","    <tr>\n","      <td>92000</td>\n","      <td>3.504400</td>\n","      <td>3.490160</td>\n","    </tr>\n","    <tr>\n","      <td>92500</td>\n","      <td>3.533200</td>\n","      <td>3.509071</td>\n","    </tr>\n","    <tr>\n","      <td>93000</td>\n","      <td>3.550800</td>\n","      <td>3.499312</td>\n","    </tr>\n","    <tr>\n","      <td>93500</td>\n","      <td>3.503500</td>\n","      <td>3.475675</td>\n","    </tr>\n","    <tr>\n","      <td>94000</td>\n","      <td>3.524800</td>\n","      <td>3.494232</td>\n","    </tr>\n","    <tr>\n","      <td>94500</td>\n","      <td>3.573500</td>\n","      <td>3.484340</td>\n","    </tr>\n","    <tr>\n","      <td>95000</td>\n","      <td>3.500900</td>\n","      <td>3.466332</td>\n","    </tr>\n","    <tr>\n","      <td>95500</td>\n","      <td>3.508500</td>\n","      <td>3.481714</td>\n","    </tr>\n","    <tr>\n","      <td>96000</td>\n","      <td>3.496100</td>\n","      <td>3.468479</td>\n","    </tr>\n","    <tr>\n","      <td>96500</td>\n","      <td>3.485500</td>\n","      <td>3.477743</td>\n","    </tr>\n","    <tr>\n","      <td>97000</td>\n","      <td>3.492500</td>\n","      <td>3.477070</td>\n","    </tr>\n","    <tr>\n","      <td>97500</td>\n","      <td>3.491000</td>\n","      <td>3.478856</td>\n","    </tr>\n","    <tr>\n","      <td>98000</td>\n","      <td>3.499600</td>\n","      <td>3.455759</td>\n","    </tr>\n","    <tr>\n","      <td>98500</td>\n","      <td>3.470000</td>\n","      <td>3.475333</td>\n","    </tr>\n","    <tr>\n","      <td>99000</td>\n","      <td>3.456500</td>\n","      <td>3.469789</td>\n","    </tr>\n","    <tr>\n","      <td>99500</td>\n","      <td>3.494100</td>\n","      <td>3.461927</td>\n","    </tr>\n","    <tr>\n","      <td>100000</td>\n","      <td>3.472900</td>\n","      <td>3.474276</td>\n","    </tr>\n","    <tr>\n","      <td>100500</td>\n","      <td>3.510000</td>\n","      <td>3.435256</td>\n","    </tr>\n","    <tr>\n","      <td>101000</td>\n","      <td>3.443000</td>\n","      <td>3.452330</td>\n","    </tr>\n","    <tr>\n","      <td>101500</td>\n","      <td>3.504200</td>\n","      <td>3.458406</td>\n","    </tr>\n","    <tr>\n","      <td>102000</td>\n","      <td>3.478600</td>\n","      <td>3.444165</td>\n","    </tr>\n","    <tr>\n","      <td>102500</td>\n","      <td>3.467100</td>\n","      <td>3.441921</td>\n","    </tr>\n","    <tr>\n","      <td>103000</td>\n","      <td>3.451200</td>\n","      <td>3.443233</td>\n","    </tr>\n","    <tr>\n","      <td>103500</td>\n","      <td>3.451900</td>\n","      <td>3.438554</td>\n","    </tr>\n","    <tr>\n","      <td>104000</td>\n","      <td>3.432500</td>\n","      <td>3.438568</td>\n","    </tr>\n","    <tr>\n","      <td>104500</td>\n","      <td>3.399700</td>\n","      <td>3.439655</td>\n","    </tr>\n","    <tr>\n","      <td>105000</td>\n","      <td>3.462200</td>\n","      <td>3.418572</td>\n","    </tr>\n","    <tr>\n","      <td>105500</td>\n","      <td>3.431500</td>\n","      <td>3.419078</td>\n","    </tr>\n","    <tr>\n","      <td>106000</td>\n","      <td>3.454900</td>\n","      <td>3.415536</td>\n","    </tr>\n","    <tr>\n","      <td>106500</td>\n","      <td>3.454900</td>\n","      <td>3.438469</td>\n","    </tr>\n","    <tr>\n","      <td>107000</td>\n","      <td>3.438600</td>\n","      <td>3.429311</td>\n","    </tr>\n","    <tr>\n","      <td>107500</td>\n","      <td>3.425800</td>\n","      <td>3.421743</td>\n","    </tr>\n","    <tr>\n","      <td>108000</td>\n","      <td>3.397600</td>\n","      <td>3.436460</td>\n","    </tr>\n","    <tr>\n","      <td>108500</td>\n","      <td>3.430000</td>\n","      <td>3.427485</td>\n","    </tr>\n","    <tr>\n","      <td>109000</td>\n","      <td>3.451500</td>\n","      <td>3.400589</td>\n","    </tr>\n","    <tr>\n","      <td>109500</td>\n","      <td>3.431500</td>\n","      <td>3.415143</td>\n","    </tr>\n","    <tr>\n","      <td>110000</td>\n","      <td>3.452600</td>\n","      <td>3.387301</td>\n","    </tr>\n","    <tr>\n","      <td>110500</td>\n","      <td>3.408000</td>\n","      <td>3.400919</td>\n","    </tr>\n","    <tr>\n","      <td>111000</td>\n","      <td>3.420500</td>\n","      <td>3.419030</td>\n","    </tr>\n","    <tr>\n","      <td>111500</td>\n","      <td>3.434900</td>\n","      <td>3.394333</td>\n","    </tr>\n","    <tr>\n","      <td>112000</td>\n","      <td>3.413600</td>\n","      <td>3.412521</td>\n","    </tr>\n","    <tr>\n","      <td>112500</td>\n","      <td>3.399100</td>\n","      <td>3.366011</td>\n","    </tr>\n","    <tr>\n","      <td>113000</td>\n","      <td>3.432800</td>\n","      <td>3.387164</td>\n","    </tr>\n","    <tr>\n","      <td>113500</td>\n","      <td>3.392400</td>\n","      <td>3.392774</td>\n","    </tr>\n","    <tr>\n","      <td>114000</td>\n","      <td>3.442100</td>\n","      <td>3.395726</td>\n","    </tr>\n","    <tr>\n","      <td>114500</td>\n","      <td>3.428400</td>\n","      <td>3.381886</td>\n","    </tr>\n","    <tr>\n","      <td>115000</td>\n","      <td>3.362900</td>\n","      <td>3.370969</td>\n","    </tr>\n","    <tr>\n","      <td>115500</td>\n","      <td>3.415100</td>\n","      <td>3.363704</td>\n","    </tr>\n","    <tr>\n","      <td>116000</td>\n","      <td>3.408900</td>\n","      <td>3.395640</td>\n","    </tr>\n","    <tr>\n","      <td>116500</td>\n","      <td>3.377400</td>\n","      <td>3.388723</td>\n","    </tr>\n","    <tr>\n","      <td>117000</td>\n","      <td>3.429800</td>\n","      <td>3.375226</td>\n","    </tr>\n","    <tr>\n","      <td>117500</td>\n","      <td>3.411000</td>\n","      <td>3.357336</td>\n","    </tr>\n","    <tr>\n","      <td>118000</td>\n","      <td>3.341200</td>\n","      <td>3.380475</td>\n","    </tr>\n","    <tr>\n","      <td>118500</td>\n","      <td>3.408400</td>\n","      <td>3.342818</td>\n","    </tr>\n","    <tr>\n","      <td>119000</td>\n","      <td>3.423900</td>\n","      <td>3.390331</td>\n","    </tr>\n","    <tr>\n","      <td>119500</td>\n","      <td>3.375500</td>\n","      <td>3.375807</td>\n","    </tr>\n","    <tr>\n","      <td>120000</td>\n","      <td>3.379200</td>\n","      <td>3.346498</td>\n","    </tr>\n","    <tr>\n","      <td>120500</td>\n","      <td>3.397400</td>\n","      <td>3.354697</td>\n","    </tr>\n","    <tr>\n","      <td>121000</td>\n","      <td>3.411000</td>\n","      <td>3.348805</td>\n","    </tr>\n","    <tr>\n","      <td>121500</td>\n","      <td>3.350200</td>\n","      <td>3.354408</td>\n","    </tr>\n","    <tr>\n","      <td>122000</td>\n","      <td>3.376500</td>\n","      <td>3.341392</td>\n","    </tr>\n","    <tr>\n","      <td>122500</td>\n","      <td>3.373900</td>\n","      <td>3.359053</td>\n","    </tr>\n","    <tr>\n","      <td>123000</td>\n","      <td>3.358200</td>\n","      <td>3.379248</td>\n","    </tr>\n","    <tr>\n","      <td>123500</td>\n","      <td>3.375400</td>\n","      <td>3.352597</td>\n","    </tr>\n","    <tr>\n","      <td>124000</td>\n","      <td>3.389300</td>\n","      <td>3.356355</td>\n","    </tr>\n","    <tr>\n","      <td>124500</td>\n","      <td>3.379700</td>\n","      <td>3.338837</td>\n","    </tr>\n","    <tr>\n","      <td>125000</td>\n","      <td>3.372200</td>\n","      <td>3.355093</td>\n","    </tr>\n","    <tr>\n","      <td>125500</td>\n","      <td>3.373800</td>\n","      <td>3.334332</td>\n","    </tr>\n","    <tr>\n","      <td>126000</td>\n","      <td>3.403300</td>\n","      <td>3.353712</td>\n","    </tr>\n","    <tr>\n","      <td>126500</td>\n","      <td>3.327200</td>\n","      <td>3.327775</td>\n","    </tr>\n","    <tr>\n","      <td>127000</td>\n","      <td>3.358200</td>\n","      <td>3.330016</td>\n","    </tr>\n","    <tr>\n","      <td>127500</td>\n","      <td>3.328200</td>\n","      <td>3.338230</td>\n","    </tr>\n","    <tr>\n","      <td>128000</td>\n","      <td>3.348700</td>\n","      <td>3.332046</td>\n","    </tr>\n","    <tr>\n","      <td>128500</td>\n","      <td>3.354100</td>\n","      <td>3.336590</td>\n","    </tr>\n","    <tr>\n","      <td>129000</td>\n","      <td>3.318700</td>\n","      <td>3.337451</td>\n","    </tr>\n","    <tr>\n","      <td>129500</td>\n","      <td>3.302800</td>\n","      <td>3.340857</td>\n","    </tr>\n","    <tr>\n","      <td>130000</td>\n","      <td>3.327900</td>\n","      <td>3.317927</td>\n","    </tr>\n","    <tr>\n","      <td>130500</td>\n","      <td>3.339000</td>\n","      <td>3.333002</td>\n","    </tr>\n","    <tr>\n","      <td>131000</td>\n","      <td>3.329700</td>\n","      <td>3.328664</td>\n","    </tr>\n","    <tr>\n","      <td>131500</td>\n","      <td>3.348900</td>\n","      <td>3.322371</td>\n","    </tr>\n","    <tr>\n","      <td>132000</td>\n","      <td>3.300500</td>\n","      <td>3.345165</td>\n","    </tr>\n","    <tr>\n","      <td>132500</td>\n","      <td>3.319600</td>\n","      <td>3.320266</td>\n","    </tr>\n","    <tr>\n","      <td>133000</td>\n","      <td>3.323500</td>\n","      <td>3.315290</td>\n","    </tr>\n","    <tr>\n","      <td>133500</td>\n","      <td>3.372500</td>\n","      <td>3.331024</td>\n","    </tr>\n","    <tr>\n","      <td>134000</td>\n","      <td>3.299300</td>\n","      <td>3.325042</td>\n","    </tr>\n","    <tr>\n","      <td>134500</td>\n","      <td>3.348800</td>\n","      <td>3.322985</td>\n","    </tr>\n","    <tr>\n","      <td>135000</td>\n","      <td>3.298900</td>\n","      <td>3.337725</td>\n","    </tr>\n","    <tr>\n","      <td>135500</td>\n","      <td>3.302400</td>\n","      <td>3.309958</td>\n","    </tr>\n","    <tr>\n","      <td>136000</td>\n","      <td>3.321300</td>\n","      <td>3.300777</td>\n","    </tr>\n","    <tr>\n","      <td>136500</td>\n","      <td>3.333100</td>\n","      <td>3.306893</td>\n","    </tr>\n","    <tr>\n","      <td>137000</td>\n","      <td>3.270800</td>\n","      <td>3.298613</td>\n","    </tr>\n","    <tr>\n","      <td>137500</td>\n","      <td>3.320600</td>\n","      <td>3.290303</td>\n","    </tr>\n","    <tr>\n","      <td>138000</td>\n","      <td>3.277200</td>\n","      <td>3.309899</td>\n","    </tr>\n","    <tr>\n","      <td>138500</td>\n","      <td>3.329900</td>\n","      <td>3.295751</td>\n","    </tr>\n","    <tr>\n","      <td>139000</td>\n","      <td>3.292800</td>\n","      <td>3.305751</td>\n","    </tr>\n","    <tr>\n","      <td>139500</td>\n","      <td>3.310500</td>\n","      <td>3.302579</td>\n","    </tr>\n","    <tr>\n","      <td>140000</td>\n","      <td>3.323000</td>\n","      <td>3.286242</td>\n","    </tr>\n","    <tr>\n","      <td>140500</td>\n","      <td>3.281300</td>\n","      <td>3.294631</td>\n","    </tr>\n","    <tr>\n","      <td>141000</td>\n","      <td>3.325500</td>\n","      <td>3.301904</td>\n","    </tr>\n","    <tr>\n","      <td>141500</td>\n","      <td>3.313300</td>\n","      <td>3.308277</td>\n","    </tr>\n","    <tr>\n","      <td>142000</td>\n","      <td>3.261900</td>\n","      <td>3.286167</td>\n","    </tr>\n","    <tr>\n","      <td>142500</td>\n","      <td>3.304600</td>\n","      <td>3.284115</td>\n","    </tr>\n","    <tr>\n","      <td>143000</td>\n","      <td>3.261700</td>\n","      <td>3.297474</td>\n","    </tr>\n","    <tr>\n","      <td>143500</td>\n","      <td>3.325000</td>\n","      <td>3.297173</td>\n","    </tr>\n","    <tr>\n","      <td>144000</td>\n","      <td>3.334800</td>\n","      <td>3.312592</td>\n","    </tr>\n","    <tr>\n","      <td>144500</td>\n","      <td>3.283300</td>\n","      <td>3.311045</td>\n","    </tr>\n","    <tr>\n","      <td>145000</td>\n","      <td>3.274700</td>\n","      <td>3.312272</td>\n","    </tr>\n","    <tr>\n","      <td>145500</td>\n","      <td>3.302800</td>\n","      <td>3.287551</td>\n","    </tr>\n","    <tr>\n","      <td>146000</td>\n","      <td>3.337700</td>\n","      <td>3.293467</td>\n","    </tr>\n","    <tr>\n","      <td>146500</td>\n","      <td>3.301900</td>\n","      <td>3.286800</td>\n","    </tr>\n","    <tr>\n","      <td>147000</td>\n","      <td>3.284400</td>\n","      <td>3.299741</td>\n","    </tr>\n","    <tr>\n","      <td>147500</td>\n","      <td>3.337800</td>\n","      <td>3.263425</td>\n","    </tr>\n","    <tr>\n","      <td>148000</td>\n","      <td>3.320900</td>\n","      <td>3.292080</td>\n","    </tr>\n","    <tr>\n","      <td>148500</td>\n","      <td>3.268400</td>\n","      <td>3.281469</td>\n","    </tr>\n","    <tr>\n","      <td>149000</td>\n","      <td>3.312800</td>\n","      <td>3.271971</td>\n","    </tr>\n","    <tr>\n","      <td>149500</td>\n","      <td>3.337200</td>\n","      <td>3.300497</td>\n","    </tr>\n","    <tr>\n","      <td>150000</td>\n","      <td>3.287100</td>\n","      <td>3.267276</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='158480' max='158480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [158480/158480 14:14:12, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.323100</td>\n","      <td>7.699655</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.628400</td>\n","      <td>7.441742</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.352600</td>\n","      <td>7.258137</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.166200</td>\n","      <td>7.083109</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>7.049500</td>\n","      <td>6.920391</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>6.919800</td>\n","      <td>6.806240</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>6.804700</td>\n","      <td>6.691088</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>6.693900</td>\n","      <td>6.530052</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.572300</td>\n","      <td>6.454288</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.476700</td>\n","      <td>6.353993</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.359600</td>\n","      <td>6.252115</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.291700</td>\n","      <td>6.134168</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.201500</td>\n","      <td>6.071229</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.128000</td>\n","      <td>5.996326</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>6.012000</td>\n","      <td>5.894029</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>5.925500</td>\n","      <td>5.849529</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>5.852700</td>\n","      <td>5.765969</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>5.837900</td>\n","      <td>5.685366</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.743300</td>\n","      <td>5.638020</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.731900</td>\n","      <td>5.596389</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.676700</td>\n","      <td>5.530405</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.578600</td>\n","      <td>5.479818</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.526600</td>\n","      <td>5.422380</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.476300</td>\n","      <td>5.370522</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.460800</td>\n","      <td>5.336691</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.416100</td>\n","      <td>5.289468</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.336200</td>\n","      <td>5.257341</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.314200</td>\n","      <td>5.214374</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.291700</td>\n","      <td>5.161703</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.237200</td>\n","      <td>5.138886</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.241100</td>\n","      <td>5.075625</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.143900</td>\n","      <td>5.044663</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>5.102500</td>\n","      <td>5.036678</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>5.056000</td>\n","      <td>5.003049</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>4.988800</td>\n","      <td>4.970918</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>5.016500</td>\n","      <td>4.914900</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>5.054500</td>\n","      <td>4.900282</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>4.980000</td>\n","      <td>4.864625</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>4.934500</td>\n","      <td>4.828434</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>4.914300</td>\n","      <td>4.807549</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>4.885300</td>\n","      <td>4.774049</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>4.867200</td>\n","      <td>4.742507</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>4.848000</td>\n","      <td>4.746023</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>4.784700</td>\n","      <td>4.699873</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>4.725700</td>\n","      <td>4.672996</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>4.770700</td>\n","      <td>4.658352</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>4.734000</td>\n","      <td>4.626236</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>4.727700</td>\n","      <td>4.616520</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>4.686500</td>\n","      <td>4.577704</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>4.630900</td>\n","      <td>4.596302</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>4.665500</td>\n","      <td>4.559931</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>4.611600</td>\n","      <td>4.542715</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>4.608400</td>\n","      <td>4.498367</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>4.562500</td>\n","      <td>4.502635</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>4.561300</td>\n","      <td>4.462436</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>4.546100</td>\n","      <td>4.478402</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>4.536200</td>\n","      <td>4.455986</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>4.503500</td>\n","      <td>4.403316</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>4.499500</td>\n","      <td>4.393417</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>4.493600</td>\n","      <td>4.389234</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>4.457300</td>\n","      <td>4.362535</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>4.470000</td>\n","      <td>4.360286</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>4.456200</td>\n","      <td>4.353082</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>4.438400</td>\n","      <td>4.341885</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>4.363900</td>\n","      <td>4.307459</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>4.325700</td>\n","      <td>4.307482</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>4.363100</td>\n","      <td>4.277255</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>4.350000</td>\n","      <td>4.260219</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>4.304200</td>\n","      <td>4.247015</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>4.251900</td>\n","      <td>4.255249</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>4.301600</td>\n","      <td>4.247949</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>4.306400</td>\n","      <td>4.214637</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>4.246800</td>\n","      <td>4.192334</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>4.222000</td>\n","      <td>4.180785</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>4.254000</td>\n","      <td>4.183067</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>4.214000</td>\n","      <td>4.185728</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>4.216200</td>\n","      <td>4.161015</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>4.241200</td>\n","      <td>4.166919</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>4.192300</td>\n","      <td>4.136303</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>4.211300</td>\n","      <td>4.141234</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>4.163200</td>\n","      <td>4.110280</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>4.194400</td>\n","      <td>4.100849</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>4.126100</td>\n","      <td>4.113945</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>4.184600</td>\n","      <td>4.072514</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>4.111200</td>\n","      <td>4.076514</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>4.132300</td>\n","      <td>4.083775</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>4.102100</td>\n","      <td>4.077248</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>4.109900</td>\n","      <td>4.052927</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>4.100000</td>\n","      <td>4.036159</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>4.050200</td>\n","      <td>4.009013</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>4.020900</td>\n","      <td>4.013702</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>4.063200</td>\n","      <td>4.008592</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>4.093600</td>\n","      <td>4.011169</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>3.999300</td>\n","      <td>3.987392</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>4.077100</td>\n","      <td>3.961033</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>3.999800</td>\n","      <td>3.963388</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>4.003700</td>\n","      <td>3.957594</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>3.998500</td>\n","      <td>3.958097</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>4.008700</td>\n","      <td>3.948853</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>4.016700</td>\n","      <td>3.935455</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>3.998100</td>\n","      <td>3.920242</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>3.976000</td>\n","      <td>3.921383</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>4.010000</td>\n","      <td>3.892124</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>3.951300</td>\n","      <td>3.891581</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>3.999000</td>\n","      <td>3.897561</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>3.958200</td>\n","      <td>3.905873</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>3.943500</td>\n","      <td>3.866236</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>4.005800</td>\n","      <td>3.874210</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>3.932100</td>\n","      <td>3.871142</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>3.900400</td>\n","      <td>3.878453</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>3.923900</td>\n","      <td>3.848500</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>3.923600</td>\n","      <td>3.846391</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>3.878600</td>\n","      <td>3.838409</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>3.910100</td>\n","      <td>3.840129</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>3.881700</td>\n","      <td>3.825138</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>3.909800</td>\n","      <td>3.829822</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>3.892400</td>\n","      <td>3.801601</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>3.916000</td>\n","      <td>3.811776</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>3.847600</td>\n","      <td>3.811334</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>3.833700</td>\n","      <td>3.804014</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>3.855700</td>\n","      <td>3.809561</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>3.840300</td>\n","      <td>3.791081</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>3.839200</td>\n","      <td>3.755806</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>3.828300</td>\n","      <td>3.752411</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>3.851400</td>\n","      <td>3.777727</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>3.838000</td>\n","      <td>3.767160</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>3.782900</td>\n","      <td>3.752557</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>3.765500</td>\n","      <td>3.729413</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>3.779300</td>\n","      <td>3.743454</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>3.755500</td>\n","      <td>3.752195</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>3.744500</td>\n","      <td>3.731699</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>3.782600</td>\n","      <td>3.712841</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>3.752000</td>\n","      <td>3.727095</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>3.712100</td>\n","      <td>3.716960</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>3.720500</td>\n","      <td>3.713384</td>\n","    </tr>\n","    <tr>\n","      <td>68000</td>\n","      <td>3.700700</td>\n","      <td>3.710413</td>\n","    </tr>\n","    <tr>\n","      <td>68500</td>\n","      <td>3.739700</td>\n","      <td>3.689943</td>\n","    </tr>\n","    <tr>\n","      <td>69000</td>\n","      <td>3.735200</td>\n","      <td>3.691814</td>\n","    </tr>\n","    <tr>\n","      <td>69500</td>\n","      <td>3.751900</td>\n","      <td>3.692579</td>\n","    </tr>\n","    <tr>\n","      <td>70000</td>\n","      <td>3.713000</td>\n","      <td>3.668799</td>\n","    </tr>\n","    <tr>\n","      <td>70500</td>\n","      <td>3.691500</td>\n","      <td>3.699093</td>\n","    </tr>\n","    <tr>\n","      <td>71000</td>\n","      <td>3.707700</td>\n","      <td>3.679486</td>\n","    </tr>\n","    <tr>\n","      <td>71500</td>\n","      <td>3.681900</td>\n","      <td>3.670473</td>\n","    </tr>\n","    <tr>\n","      <td>72000</td>\n","      <td>3.719400</td>\n","      <td>3.671936</td>\n","    </tr>\n","    <tr>\n","      <td>72500</td>\n","      <td>3.729200</td>\n","      <td>3.645765</td>\n","    </tr>\n","    <tr>\n","      <td>73000</td>\n","      <td>3.652400</td>\n","      <td>3.659861</td>\n","    </tr>\n","    <tr>\n","      <td>73500</td>\n","      <td>3.680400</td>\n","      <td>3.651311</td>\n","    </tr>\n","    <tr>\n","      <td>74000</td>\n","      <td>3.694700</td>\n","      <td>3.631735</td>\n","    </tr>\n","    <tr>\n","      <td>74500</td>\n","      <td>3.698600</td>\n","      <td>3.632925</td>\n","    </tr>\n","    <tr>\n","      <td>75000</td>\n","      <td>3.654300</td>\n","      <td>3.625872</td>\n","    </tr>\n","    <tr>\n","      <td>75500</td>\n","      <td>3.718200</td>\n","      <td>3.638897</td>\n","    </tr>\n","    <tr>\n","      <td>76000</td>\n","      <td>3.707900</td>\n","      <td>3.627610</td>\n","    </tr>\n","    <tr>\n","      <td>76500</td>\n","      <td>3.646700</td>\n","      <td>3.612916</td>\n","    </tr>\n","    <tr>\n","      <td>77000</td>\n","      <td>3.680000</td>\n","      <td>3.614781</td>\n","    </tr>\n","    <tr>\n","      <td>77500</td>\n","      <td>3.668600</td>\n","      <td>3.622719</td>\n","    </tr>\n","    <tr>\n","      <td>78000</td>\n","      <td>3.657300</td>\n","      <td>3.615793</td>\n","    </tr>\n","    <tr>\n","      <td>78500</td>\n","      <td>3.588800</td>\n","      <td>3.602563</td>\n","    </tr>\n","    <tr>\n","      <td>79000</td>\n","      <td>3.666400</td>\n","      <td>3.594930</td>\n","    </tr>\n","    <tr>\n","      <td>79500</td>\n","      <td>3.627600</td>\n","      <td>3.594881</td>\n","    </tr>\n","    <tr>\n","      <td>80000</td>\n","      <td>3.595200</td>\n","      <td>3.591403</td>\n","    </tr>\n","    <tr>\n","      <td>80500</td>\n","      <td>3.668500</td>\n","      <td>3.579723</td>\n","    </tr>\n","    <tr>\n","      <td>81000</td>\n","      <td>3.691500</td>\n","      <td>3.578439</td>\n","    </tr>\n","    <tr>\n","      <td>81500</td>\n","      <td>3.648800</td>\n","      <td>3.595245</td>\n","    </tr>\n","    <tr>\n","      <td>82000</td>\n","      <td>3.643300</td>\n","      <td>3.572337</td>\n","    </tr>\n","    <tr>\n","      <td>82500</td>\n","      <td>3.606400</td>\n","      <td>3.562537</td>\n","    </tr>\n","    <tr>\n","      <td>83000</td>\n","      <td>3.594700</td>\n","      <td>3.564169</td>\n","    </tr>\n","    <tr>\n","      <td>83500</td>\n","      <td>3.608000</td>\n","      <td>3.563971</td>\n","    </tr>\n","    <tr>\n","      <td>84000</td>\n","      <td>3.620600</td>\n","      <td>3.573940</td>\n","    </tr>\n","    <tr>\n","      <td>84500</td>\n","      <td>3.628400</td>\n","      <td>3.560497</td>\n","    </tr>\n","    <tr>\n","      <td>85000</td>\n","      <td>3.615600</td>\n","      <td>3.574631</td>\n","    </tr>\n","    <tr>\n","      <td>85500</td>\n","      <td>3.595700</td>\n","      <td>3.553571</td>\n","    </tr>\n","    <tr>\n","      <td>86000</td>\n","      <td>3.619800</td>\n","      <td>3.553386</td>\n","    </tr>\n","    <tr>\n","      <td>86500</td>\n","      <td>3.586500</td>\n","      <td>3.533318</td>\n","    </tr>\n","    <tr>\n","      <td>87000</td>\n","      <td>3.555900</td>\n","      <td>3.535371</td>\n","    </tr>\n","    <tr>\n","      <td>87500</td>\n","      <td>3.626800</td>\n","      <td>3.541096</td>\n","    </tr>\n","    <tr>\n","      <td>88000</td>\n","      <td>3.551600</td>\n","      <td>3.529006</td>\n","    </tr>\n","    <tr>\n","      <td>88500</td>\n","      <td>3.511200</td>\n","      <td>3.540694</td>\n","    </tr>\n","    <tr>\n","      <td>89000</td>\n","      <td>3.571600</td>\n","      <td>3.534573</td>\n","    </tr>\n","    <tr>\n","      <td>89500</td>\n","      <td>3.550600</td>\n","      <td>3.508835</td>\n","    </tr>\n","    <tr>\n","      <td>90000</td>\n","      <td>3.579600</td>\n","      <td>3.509177</td>\n","    </tr>\n","    <tr>\n","      <td>90500</td>\n","      <td>3.593200</td>\n","      <td>3.515264</td>\n","    </tr>\n","    <tr>\n","      <td>91000</td>\n","      <td>3.519200</td>\n","      <td>3.525867</td>\n","    </tr>\n","    <tr>\n","      <td>91500</td>\n","      <td>3.544400</td>\n","      <td>3.503102</td>\n","    </tr>\n","    <tr>\n","      <td>92000</td>\n","      <td>3.504400</td>\n","      <td>3.490160</td>\n","    </tr>\n","    <tr>\n","      <td>92500</td>\n","      <td>3.533200</td>\n","      <td>3.509071</td>\n","    </tr>\n","    <tr>\n","      <td>93000</td>\n","      <td>3.550800</td>\n","      <td>3.499312</td>\n","    </tr>\n","    <tr>\n","      <td>93500</td>\n","      <td>3.503500</td>\n","      <td>3.475675</td>\n","    </tr>\n","    <tr>\n","      <td>94000</td>\n","      <td>3.524800</td>\n","      <td>3.494232</td>\n","    </tr>\n","    <tr>\n","      <td>94500</td>\n","      <td>3.573500</td>\n","      <td>3.484340</td>\n","    </tr>\n","    <tr>\n","      <td>95000</td>\n","      <td>3.500900</td>\n","      <td>3.466332</td>\n","    </tr>\n","    <tr>\n","      <td>95500</td>\n","      <td>3.508500</td>\n","      <td>3.481714</td>\n","    </tr>\n","    <tr>\n","      <td>96000</td>\n","      <td>3.496100</td>\n","      <td>3.468479</td>\n","    </tr>\n","    <tr>\n","      <td>96500</td>\n","      <td>3.485500</td>\n","      <td>3.477743</td>\n","    </tr>\n","    <tr>\n","      <td>97000</td>\n","      <td>3.492500</td>\n","      <td>3.477070</td>\n","    </tr>\n","    <tr>\n","      <td>97500</td>\n","      <td>3.491000</td>\n","      <td>3.478856</td>\n","    </tr>\n","    <tr>\n","      <td>98000</td>\n","      <td>3.499600</td>\n","      <td>3.455759</td>\n","    </tr>\n","    <tr>\n","      <td>98500</td>\n","      <td>3.470000</td>\n","      <td>3.475333</td>\n","    </tr>\n","    <tr>\n","      <td>99000</td>\n","      <td>3.456500</td>\n","      <td>3.469789</td>\n","    </tr>\n","    <tr>\n","      <td>99500</td>\n","      <td>3.494100</td>\n","      <td>3.461927</td>\n","    </tr>\n","    <tr>\n","      <td>100000</td>\n","      <td>3.472900</td>\n","      <td>3.474276</td>\n","    </tr>\n","    <tr>\n","      <td>100500</td>\n","      <td>3.510000</td>\n","      <td>3.435256</td>\n","    </tr>\n","    <tr>\n","      <td>101000</td>\n","      <td>3.443000</td>\n","      <td>3.452330</td>\n","    </tr>\n","    <tr>\n","      <td>101500</td>\n","      <td>3.504200</td>\n","      <td>3.458406</td>\n","    </tr>\n","    <tr>\n","      <td>102000</td>\n","      <td>3.478600</td>\n","      <td>3.444165</td>\n","    </tr>\n","    <tr>\n","      <td>102500</td>\n","      <td>3.467100</td>\n","      <td>3.441921</td>\n","    </tr>\n","    <tr>\n","      <td>103000</td>\n","      <td>3.451200</td>\n","      <td>3.443233</td>\n","    </tr>\n","    <tr>\n","      <td>103500</td>\n","      <td>3.451900</td>\n","      <td>3.438554</td>\n","    </tr>\n","    <tr>\n","      <td>104000</td>\n","      <td>3.432500</td>\n","      <td>3.438568</td>\n","    </tr>\n","    <tr>\n","      <td>104500</td>\n","      <td>3.399700</td>\n","      <td>3.439655</td>\n","    </tr>\n","    <tr>\n","      <td>105000</td>\n","      <td>3.462200</td>\n","      <td>3.418572</td>\n","    </tr>\n","    <tr>\n","      <td>105500</td>\n","      <td>3.431500</td>\n","      <td>3.419078</td>\n","    </tr>\n","    <tr>\n","      <td>106000</td>\n","      <td>3.454900</td>\n","      <td>3.415536</td>\n","    </tr>\n","    <tr>\n","      <td>106500</td>\n","      <td>3.454900</td>\n","      <td>3.438469</td>\n","    </tr>\n","    <tr>\n","      <td>107000</td>\n","      <td>3.438600</td>\n","      <td>3.429311</td>\n","    </tr>\n","    <tr>\n","      <td>107500</td>\n","      <td>3.425800</td>\n","      <td>3.421743</td>\n","    </tr>\n","    <tr>\n","      <td>108000</td>\n","      <td>3.397600</td>\n","      <td>3.436460</td>\n","    </tr>\n","    <tr>\n","      <td>108500</td>\n","      <td>3.430000</td>\n","      <td>3.427485</td>\n","    </tr>\n","    <tr>\n","      <td>109000</td>\n","      <td>3.451500</td>\n","      <td>3.400589</td>\n","    </tr>\n","    <tr>\n","      <td>109500</td>\n","      <td>3.431500</td>\n","      <td>3.415143</td>\n","    </tr>\n","    <tr>\n","      <td>110000</td>\n","      <td>3.452600</td>\n","      <td>3.387301</td>\n","    </tr>\n","    <tr>\n","      <td>110500</td>\n","      <td>3.408000</td>\n","      <td>3.400919</td>\n","    </tr>\n","    <tr>\n","      <td>111000</td>\n","      <td>3.420500</td>\n","      <td>3.419030</td>\n","    </tr>\n","    <tr>\n","      <td>111500</td>\n","      <td>3.434900</td>\n","      <td>3.394333</td>\n","    </tr>\n","    <tr>\n","      <td>112000</td>\n","      <td>3.413600</td>\n","      <td>3.412521</td>\n","    </tr>\n","    <tr>\n","      <td>112500</td>\n","      <td>3.399100</td>\n","      <td>3.366011</td>\n","    </tr>\n","    <tr>\n","      <td>113000</td>\n","      <td>3.432800</td>\n","      <td>3.387164</td>\n","    </tr>\n","    <tr>\n","      <td>113500</td>\n","      <td>3.392400</td>\n","      <td>3.392774</td>\n","    </tr>\n","    <tr>\n","      <td>114000</td>\n","      <td>3.442100</td>\n","      <td>3.395726</td>\n","    </tr>\n","    <tr>\n","      <td>114500</td>\n","      <td>3.428400</td>\n","      <td>3.381886</td>\n","    </tr>\n","    <tr>\n","      <td>115000</td>\n","      <td>3.362900</td>\n","      <td>3.370969</td>\n","    </tr>\n","    <tr>\n","      <td>115500</td>\n","      <td>3.415100</td>\n","      <td>3.363704</td>\n","    </tr>\n","    <tr>\n","      <td>116000</td>\n","      <td>3.408900</td>\n","      <td>3.395640</td>\n","    </tr>\n","    <tr>\n","      <td>116500</td>\n","      <td>3.377400</td>\n","      <td>3.388723</td>\n","    </tr>\n","    <tr>\n","      <td>117000</td>\n","      <td>3.429800</td>\n","      <td>3.375226</td>\n","    </tr>\n","    <tr>\n","      <td>117500</td>\n","      <td>3.411000</td>\n","      <td>3.357336</td>\n","    </tr>\n","    <tr>\n","      <td>118000</td>\n","      <td>3.341200</td>\n","      <td>3.380475</td>\n","    </tr>\n","    <tr>\n","      <td>118500</td>\n","      <td>3.408400</td>\n","      <td>3.342818</td>\n","    </tr>\n","    <tr>\n","      <td>119000</td>\n","      <td>3.423900</td>\n","      <td>3.390331</td>\n","    </tr>\n","    <tr>\n","      <td>119500</td>\n","      <td>3.375500</td>\n","      <td>3.375807</td>\n","    </tr>\n","    <tr>\n","      <td>120000</td>\n","      <td>3.379200</td>\n","      <td>3.346498</td>\n","    </tr>\n","    <tr>\n","      <td>120500</td>\n","      <td>3.397400</td>\n","      <td>3.354697</td>\n","    </tr>\n","    <tr>\n","      <td>121000</td>\n","      <td>3.411000</td>\n","      <td>3.348805</td>\n","    </tr>\n","    <tr>\n","      <td>121500</td>\n","      <td>3.350200</td>\n","      <td>3.354408</td>\n","    </tr>\n","    <tr>\n","      <td>122000</td>\n","      <td>3.376500</td>\n","      <td>3.341392</td>\n","    </tr>\n","    <tr>\n","      <td>122500</td>\n","      <td>3.373900</td>\n","      <td>3.359053</td>\n","    </tr>\n","    <tr>\n","      <td>123000</td>\n","      <td>3.358200</td>\n","      <td>3.379248</td>\n","    </tr>\n","    <tr>\n","      <td>123500</td>\n","      <td>3.375400</td>\n","      <td>3.352597</td>\n","    </tr>\n","    <tr>\n","      <td>124000</td>\n","      <td>3.389300</td>\n","      <td>3.356355</td>\n","    </tr>\n","    <tr>\n","      <td>124500</td>\n","      <td>3.379700</td>\n","      <td>3.338837</td>\n","    </tr>\n","    <tr>\n","      <td>125000</td>\n","      <td>3.372200</td>\n","      <td>3.355093</td>\n","    </tr>\n","    <tr>\n","      <td>125500</td>\n","      <td>3.373800</td>\n","      <td>3.334332</td>\n","    </tr>\n","    <tr>\n","      <td>126000</td>\n","      <td>3.403300</td>\n","      <td>3.353712</td>\n","    </tr>\n","    <tr>\n","      <td>126500</td>\n","      <td>3.327200</td>\n","      <td>3.327775</td>\n","    </tr>\n","    <tr>\n","      <td>127000</td>\n","      <td>3.358200</td>\n","      <td>3.330016</td>\n","    </tr>\n","    <tr>\n","      <td>127500</td>\n","      <td>3.328200</td>\n","      <td>3.338230</td>\n","    </tr>\n","    <tr>\n","      <td>128000</td>\n","      <td>3.348700</td>\n","      <td>3.332046</td>\n","    </tr>\n","    <tr>\n","      <td>128500</td>\n","      <td>3.354100</td>\n","      <td>3.336590</td>\n","    </tr>\n","    <tr>\n","      <td>129000</td>\n","      <td>3.318700</td>\n","      <td>3.337451</td>\n","    </tr>\n","    <tr>\n","      <td>129500</td>\n","      <td>3.302800</td>\n","      <td>3.340857</td>\n","    </tr>\n","    <tr>\n","      <td>130000</td>\n","      <td>3.327900</td>\n","      <td>3.317927</td>\n","    </tr>\n","    <tr>\n","      <td>130500</td>\n","      <td>3.339000</td>\n","      <td>3.333002</td>\n","    </tr>\n","    <tr>\n","      <td>131000</td>\n","      <td>3.329700</td>\n","      <td>3.328664</td>\n","    </tr>\n","    <tr>\n","      <td>131500</td>\n","      <td>3.348900</td>\n","      <td>3.322371</td>\n","    </tr>\n","    <tr>\n","      <td>132000</td>\n","      <td>3.300500</td>\n","      <td>3.345165</td>\n","    </tr>\n","    <tr>\n","      <td>132500</td>\n","      <td>3.319600</td>\n","      <td>3.320266</td>\n","    </tr>\n","    <tr>\n","      <td>133000</td>\n","      <td>3.323500</td>\n","      <td>3.315290</td>\n","    </tr>\n","    <tr>\n","      <td>133500</td>\n","      <td>3.372500</td>\n","      <td>3.331024</td>\n","    </tr>\n","    <tr>\n","      <td>134000</td>\n","      <td>3.299300</td>\n","      <td>3.325042</td>\n","    </tr>\n","    <tr>\n","      <td>134500</td>\n","      <td>3.348800</td>\n","      <td>3.322985</td>\n","    </tr>\n","    <tr>\n","      <td>135000</td>\n","      <td>3.298900</td>\n","      <td>3.337725</td>\n","    </tr>\n","    <tr>\n","      <td>135500</td>\n","      <td>3.302400</td>\n","      <td>3.309958</td>\n","    </tr>\n","    <tr>\n","      <td>136000</td>\n","      <td>3.321300</td>\n","      <td>3.300777</td>\n","    </tr>\n","    <tr>\n","      <td>136500</td>\n","      <td>3.333100</td>\n","      <td>3.306893</td>\n","    </tr>\n","    <tr>\n","      <td>137000</td>\n","      <td>3.270800</td>\n","      <td>3.298613</td>\n","    </tr>\n","    <tr>\n","      <td>137500</td>\n","      <td>3.320600</td>\n","      <td>3.290303</td>\n","    </tr>\n","    <tr>\n","      <td>138000</td>\n","      <td>3.277200</td>\n","      <td>3.309899</td>\n","    </tr>\n","    <tr>\n","      <td>138500</td>\n","      <td>3.329900</td>\n","      <td>3.295751</td>\n","    </tr>\n","    <tr>\n","      <td>139000</td>\n","      <td>3.292800</td>\n","      <td>3.305751</td>\n","    </tr>\n","    <tr>\n","      <td>139500</td>\n","      <td>3.310500</td>\n","      <td>3.302579</td>\n","    </tr>\n","    <tr>\n","      <td>140000</td>\n","      <td>3.323000</td>\n","      <td>3.286242</td>\n","    </tr>\n","    <tr>\n","      <td>140500</td>\n","      <td>3.281300</td>\n","      <td>3.294631</td>\n","    </tr>\n","    <tr>\n","      <td>141000</td>\n","      <td>3.325500</td>\n","      <td>3.301904</td>\n","    </tr>\n","    <tr>\n","      <td>141500</td>\n","      <td>3.313300</td>\n","      <td>3.308277</td>\n","    </tr>\n","    <tr>\n","      <td>142000</td>\n","      <td>3.261900</td>\n","      <td>3.286167</td>\n","    </tr>\n","    <tr>\n","      <td>142500</td>\n","      <td>3.304600</td>\n","      <td>3.284115</td>\n","    </tr>\n","    <tr>\n","      <td>143000</td>\n","      <td>3.261700</td>\n","      <td>3.297474</td>\n","    </tr>\n","    <tr>\n","      <td>143500</td>\n","      <td>3.325000</td>\n","      <td>3.297173</td>\n","    </tr>\n","    <tr>\n","      <td>144000</td>\n","      <td>3.334800</td>\n","      <td>3.312592</td>\n","    </tr>\n","    <tr>\n","      <td>144500</td>\n","      <td>3.283300</td>\n","      <td>3.311045</td>\n","    </tr>\n","    <tr>\n","      <td>145000</td>\n","      <td>3.274700</td>\n","      <td>3.312272</td>\n","    </tr>\n","    <tr>\n","      <td>145500</td>\n","      <td>3.302800</td>\n","      <td>3.287551</td>\n","    </tr>\n","    <tr>\n","      <td>146000</td>\n","      <td>3.337700</td>\n","      <td>3.293467</td>\n","    </tr>\n","    <tr>\n","      <td>146500</td>\n","      <td>3.301900</td>\n","      <td>3.286800</td>\n","    </tr>\n","    <tr>\n","      <td>147000</td>\n","      <td>3.284400</td>\n","      <td>3.299741</td>\n","    </tr>\n","    <tr>\n","      <td>147500</td>\n","      <td>3.337800</td>\n","      <td>3.263425</td>\n","    </tr>\n","    <tr>\n","      <td>148000</td>\n","      <td>3.320900</td>\n","      <td>3.292080</td>\n","    </tr>\n","    <tr>\n","      <td>148500</td>\n","      <td>3.268400</td>\n","      <td>3.281469</td>\n","    </tr>\n","    <tr>\n","      <td>149000</td>\n","      <td>3.312800</td>\n","      <td>3.271971</td>\n","    </tr>\n","    <tr>\n","      <td>149500</td>\n","      <td>3.337200</td>\n","      <td>3.300497</td>\n","    </tr>\n","    <tr>\n","      <td>150000</td>\n","      <td>3.287100</td>\n","      <td>3.267276</td>\n","    </tr>\n","    <tr>\n","      <td>150500</td>\n","      <td>3.275400</td>\n","      <td>3.288624</td>\n","    </tr>\n","    <tr>\n","      <td>151000</td>\n","      <td>3.260300</td>\n","      <td>3.297041</td>\n","    </tr>\n","    <tr>\n","      <td>151500</td>\n","      <td>3.277700</td>\n","      <td>3.273371</td>\n","    </tr>\n","    <tr>\n","      <td>152000</td>\n","      <td>3.326300</td>\n","      <td>3.273046</td>\n","    </tr>\n","    <tr>\n","      <td>152500</td>\n","      <td>3.264100</td>\n","      <td>3.300606</td>\n","    </tr>\n","    <tr>\n","      <td>153000</td>\n","      <td>3.294500</td>\n","      <td>3.299460</td>\n","    </tr>\n","    <tr>\n","      <td>153500</td>\n","      <td>3.269500</td>\n","      <td>3.269929</td>\n","    </tr>\n","    <tr>\n","      <td>154000</td>\n","      <td>3.282700</td>\n","      <td>3.277236</td>\n","    </tr>\n","    <tr>\n","      <td>154500</td>\n","      <td>3.279100</td>\n","      <td>3.286665</td>\n","    </tr>\n","    <tr>\n","      <td>155000</td>\n","      <td>3.327900</td>\n","      <td>3.286550</td>\n","    </tr>\n","    <tr>\n","      <td>155500</td>\n","      <td>3.227000</td>\n","      <td>3.283090</td>\n","    </tr>\n","    <tr>\n","      <td>156000</td>\n","      <td>3.303900</td>\n","      <td>3.304374</td>\n","    </tr>\n","    <tr>\n","      <td>156500</td>\n","      <td>3.256500</td>\n","      <td>3.281978</td>\n","    </tr>\n","    <tr>\n","      <td>157000</td>\n","      <td>3.278000</td>\n","      <td>3.287987</td>\n","    </tr>\n","    <tr>\n","      <td>157500</td>\n","      <td>3.303200</td>\n","      <td>3.272379</td>\n","    </tr>\n","    <tr>\n","      <td>158000</td>\n","      <td>3.275200</td>\n","      <td>3.290816</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 53300\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_111k/final\n","Configuration saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_111k/final/config.json\n","Model weights saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_111k/final/pytorch_model.bin\n","tokenizing:   0%|          | 0/1038946 [00:00<?, ?it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3c85038133d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_description_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"main_exp_v1.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/My Drive/digital_breakthrough/task_3/src/train.py\u001b[0m in \u001b[0;36mtrain_description_model\u001b[0;34m(config_name, skip_fitting_tokenizer, skip_fitting_lm_model, skip_fitting_classification_model)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mtokenizer_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 max_len=args_classification_model_max_len)\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypology\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             model = distilbert_model(\n","\u001b[0;32m/content/drive/My Drive/digital_breakthrough/task_3/src/preprocess.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(texts, tokenizer_path, tokenizer, max_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenizing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         encoded = tokenizer.encode_plus(text,\n\u001b[0m\u001b[1;32m     26\u001b[0m                                         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                         \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'LMTokenizer' object has no attribute 'encode_plus'"]}]},{"cell_type":"code","metadata":{"id":"XcG7e0I3T2ft","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628265164125,"user_tz":-180,"elapsed":6224474,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"dea75a65-47f0-4172-b995-0ebf64480225"},"source":["train_description_model(\"main_exp_v1.yaml\", skip_fitting_tokenizer=True, skip_fitting_lm_model=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                               –ø—Ä–æ—á–∏–µ\n","12                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","13                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","14                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/train_data.csv 15\n","/content/drive/MyDrive/digital_breakthrough/task_3/models/tokenizers/wordpiece_111k.json\n"],"name":"stdout"},{"output_type":"stream","text":["tokenizing:   0%|          | 0/1038946 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing:   0%|          | 394/1038946 [00:00<04:23, 3939.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3/models/tokenizers/wordpiece_111k.json\n"],"name":"stdout"},{"output_type":"stream","text":["tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1038946/1038946 [02:31<00:00, 6840.02it/s]\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","distilbert (TFDistilBertMainLay TFBaseModelOutput(la 76066304    input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 512)          0           distilbert[0][7]                 \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 15)           7695        global_average_pooling1d[0][0]   \n","==================================================================================================\n","Total params: 76,073,999\n","Trainable params: 76,073,999\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","14611/14611 [==============================] - 1337s 91ms/step - loss: 0.5735 - f1_score: nan - val_loss: 0.8634 - val_f1_score: nan\n","Epoch 2/5\n","14611/14611 [==============================] - 1324s 91ms/step - loss: 0.4229 - f1_score: 0.8593 - val_loss: 0.6048 - val_f1_score: nan\n","Epoch 3/5\n","14611/14611 [==============================] - 1324s 91ms/step - loss: 0.3566 - f1_score: 0.8811 - val_loss: 0.6724 - val_f1_score: nan\n","Epoch 4/5\n","14611/14611 [==============================] - 1323s 91ms/step - loss: 0.3078 - f1_score: 0.8974 - val_loss: 0.7389 - val_f1_score: nan\n","Epoch 5/5\n","14611/14611 [==============================] - 1324s 91ms/step - loss: 0.2669 - f1_score: 0.9107 - val_loss: 0.7512 - val_f1_score: nan\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fcddae98410>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fcddf5ca310>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fcde1952b90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fcddae98f10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fcde03cbb90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fcde03f04d0>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/distilbert_wordpiece_111k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/distilbert_wordpiece_111k/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycATRkJo5eN0","executionInfo":{"status":"ok","timestamp":1628331581494,"user_tz":-180,"elapsed":2400324,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"ab312b10-55bc-442e-e5b5-5d403e65c007"},"source":["train_description_model(\"main_exp_v2.yaml\", skip_fitting_tokenizer=True, skip_fitting_lm_model=True)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v12\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                               –ø—Ä–æ—á–∏–µ\n","12                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","13                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","14                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v12/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v12/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v12/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v12/train_data.csv 15\n","/content/drive/MyDrive/digital_breakthrough/task_3/models/tokenizers/wordpiece_111k.json\n"],"name":"stdout"},{"output_type":"stream","text":["tokenizing:   0%|          | 0/1038946 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing:   0%|          | 409/1038946 [00:00<04:13, 4088.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3/models/tokenizers/wordpiece_111k.json\n"],"name":"stdout"},{"output_type":"stream","text":["tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1038946/1038946 [02:33<00:00, 6769.66it/s]\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","distilbert (TFDistilBertMainLay TFBaseModelOutput(la 76066304    input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 32, 128)      295424      distilbert[0][7]                 \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n","                                                                 global_max_pooling1d[0][0]       \n","__________________________________________________________________________________________________\n","dropout_19 (Dropout)            (None, 256)          0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 15)           3855        dropout_19[0][0]                 \n","==================================================================================================\n","Total params: 76,365,583\n","Trainable params: 299,279\n","Non-trainable params: 76,066,304\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","16234/16234 [==============================] - 639s 39ms/step - loss: 1.1820 - f1_score: nan\n","Epoch 2/5\n","16234/16234 [==============================] - 624s 38ms/step - loss: 0.8439 - f1_score: 0.7226\n","Epoch 3/5\n","16234/16234 [==============================] - 622s 38ms/step - loss: 0.7741 - f1_score: 0.7468\n","Epoch 4/5\n","16234/16234 [==============================] - 621s 38ms/step - loss: 0.7338 - f1_score: 0.7601\n","Epoch 5/5\n","16234/16234 [==============================] - 624s 38ms/step - loss: 0.7061 - f1_score: 0.7698\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe72b3c4e10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe73c77fa50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe72e4ea3d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe72e50bd50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe726f55810>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe7138d6390>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_layer_call_and_return_conditional_losses while saving (showing 5 of 425). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/distilbert_wordpiece_111k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/distilbert_wordpiece_111k/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VvrcxfWGuyPe"},"source":["path_item, path_item_train, path_item_test, path_input, len_cat = (\n","    '/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name.txt',\n","    '/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name_train.txt',\n","    '/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/item_name_valid.txt',\n","    '/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v7/train_data.csv',\n","    15\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9h5f-MLvvI1X"},"source":[""],"execution_count":null,"outputs":[]}]}