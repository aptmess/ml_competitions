{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"–ö–æ–ø–∏—è one_click.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2JQuEQqEQ5m0"},"source":["# Fitting Description"]},{"cell_type":"markdown","metadata":{"id":"JSG4Ngd5Q92E"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KDwqlJw0cn7","executionInfo":{"status":"ok","timestamp":1628265240114,"user_tz":-180,"elapsed":16331,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"7b7f7d72-702a-47c9-d788-e16dd0abc88a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K5m4OpF7PEbo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628265240115,"user_tz":-180,"elapsed":12,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"d7d5de41-a5f6-4919-c2a8-ccb236b3e3e9"},"source":["cd /content/drive/MyDrive/digital_breakthrough/task_3"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g57naKGtkvGZ"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFPM4lPrk7-Q","executionInfo":{"status":"ok","timestamp":1628265271621,"user_tz":-180,"elapsed":403,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"d62c97a0-a86a-4d88-aa44-6c4dd777d67c"},"source":["cd /content/drive/MyDrive/digital_breakthrough/task_3"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OJCtJQLslRIO"},"source":["import logging\n","from src.predict import prediction\n","from src.train import train_description_model\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","log = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yl8lgBavld9v"},"source":["from src.predict import prediction\n","from airotica.airotica import detectron"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYeWI0quMfSF","executionInfo":{"status":"ok","timestamp":1628265309681,"user_tz":-180,"elapsed":26180,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"ff6945f6-af16-4150-d7c0-87d36a7fa3aa"},"source":["pred_desc = prediction(config_name=\"predict_full.yaml\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tokenizing:   0%|          | 0/676 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676/676 [00:00<00:00, 6187.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","{\"asctime\": \"2021-08-06 15:55:04\", \"name\": \"tensorflow\", \"filename\": \"deprecation.py\", \"levelname\": \"WARNING\", \"message\": \"From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\\nInstructions for updating:\\nThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\"}\n","3/3 [==============================] - 3s 73ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDmAPcukIaUw","executionInfo":{"status":"ok","timestamp":1628265617027,"user_tz":-180,"elapsed":270141,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"29b62ee8-71d6-40a5-d1ac-e4d7e79a4c52"},"source":["images_prediction_2 = detectron(config_name='config5.yml', \n","                                save_path='./sub/_pred_new.csv',\n","                                model_path='./EXPERIMENTS/full_res/model_0001_0.857586.pth',\n","                                model_type='resnext101_32x8d',\n","                                save_result=True,\n","                                return_result=True,\n","                                use_description=True\n","                              )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/1223 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1223/1223 [04:04<00:00,  5.01it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9PAEuXi9Mtzd"},"source":["import pandas as pd\n","\n","from definitions import DATA_PATH, SUB_PATH\n","\n","train = pd.read_csv(DATA_PATH / 'train.csv')\n","\n","sample_submission = pd.read_csv(DATA_PATH / 'sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDGTj0NiNvsL"},"source":["train_labels = train.typology.unique()\n","typology_to_label = dict(\n","    zip(\n","        sorted(train_labels),\n","        range(\n","            len(train_labels)\n","        )\n","    )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCQ50EIWNxzD"},"source":["label_to_typology = {v: k for k, v in typology_to_label.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Be1WbCRtOJnD"},"source":["MAPPER = {0: 0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, \n","          7:7, 8:8, 9:9, 10:10, 11:12, 12:13, 13:14,\n","          -1: -1\n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0EfhOb5M7Nt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628265627334,"user_tz":-180,"elapsed":296,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"6d05bb0b-2869-4b02-f94f-3fa7febdb1ee"},"source":["tst = (\n","    images_prediction_2[images_prediction_2.sign_0 != -1]['sign_0']\n","    .map(MAPPER)\n","    .map(label_to_typology)\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{\"asctime\": \"2021-08-06 16:00:27\", \"name\": \"numexpr.utils\", \"filename\": \"utils.py\", \"levelname\": \"INFO\", \"message\": \"NumExpr defaulting to 4 threads.\"}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5b5QpTaxOYPi"},"source":["sample_submission.loc[pred_desc.index, 'typology'] = pred_desc.values\n","sample_submission.loc[tst.index, 'typology'] = tst.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJn8Ca5WO9IR"},"source":["sample_submission.to_csv(SUB_PATH / 'imnlp.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2b2njVFxSBtD"},"source":["### ROBERTA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8YCS5jLcSCTh","executionInfo":{"status":"ok","timestamp":1628092653802,"user_tz":-180,"elapsed":4793853,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"23f04047-6c39-418e-ab64-271bbeda4ebb"},"source":["train_description_model(config_name='exp_v2:roberta.yaml')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","12                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","13                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/train_data.csv 14\n"],"name":"stdout"},{"output_type":"stream","text":["using `logging_steps` to initialize `eval_steps` to 500\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Creating features from dataset file at /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_train.txt\n","Creating features from dataset file at /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_valid.txt\n","***** Running training *****\n","  Num examples = 147319\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23020\n"],"name":"stderr"},{"output_type":"stream","text":["<transformers.data.datasets.language_modeling.LineByLineTextDataset object at 0x7f72c33b1210>\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='16326' max='23020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [16326/23020 1:27:08 < 35:44, 3.12 it/s, Epoch 3.55/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.313100</td>\n","      <td>7.916295</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.845800</td>\n","      <td>7.702612</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.639700</td>\n","      <td>7.642996</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.512000</td>\n","      <td>7.499543</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>7.383200</td>\n","      <td>7.377310</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>7.246600</td>\n","      <td>7.264523</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>7.107100</td>\n","      <td>7.073224</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>7.001700</td>\n","      <td>6.913572</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.875100</td>\n","      <td>6.713515</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.704000</td>\n","      <td>6.724208</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.601700</td>\n","      <td>6.623839</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.589900</td>\n","      <td>6.480356</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.440400</td>\n","      <td>6.408947</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.317200</td>\n","      <td>6.334692</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>6.215600</td>\n","      <td>6.309314</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>6.240400</td>\n","      <td>6.192551</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>6.175300</td>\n","      <td>6.138880</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>6.109300</td>\n","      <td>6.114434</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.999200</td>\n","      <td>6.050646</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.942900</td>\n","      <td>5.947344</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.891400</td>\n","      <td>5.922942</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.809800</td>\n","      <td>5.858764</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.874800</td>\n","      <td>5.856677</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.725000</td>\n","      <td>5.697091</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.748900</td>\n","      <td>5.705881</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.691100</td>\n","      <td>5.734134</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.617500</td>\n","      <td>5.678579</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.623400</td>\n","      <td>5.607348</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.555700</td>\n","      <td>5.624840</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.584100</td>\n","      <td>5.637661</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.474600</td>\n","      <td>5.607300</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.479700</td>\n","      <td>5.586270</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='23020' max='23020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23020/23020 2:03:30, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.313100</td>\n","      <td>7.916295</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.845800</td>\n","      <td>7.702612</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.639700</td>\n","      <td>7.642996</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.512000</td>\n","      <td>7.499543</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>7.383200</td>\n","      <td>7.377310</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>7.246600</td>\n","      <td>7.264523</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>7.107100</td>\n","      <td>7.073224</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>7.001700</td>\n","      <td>6.913572</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.875100</td>\n","      <td>6.713515</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.704000</td>\n","      <td>6.724208</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.601700</td>\n","      <td>6.623839</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.589900</td>\n","      <td>6.480356</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.440400</td>\n","      <td>6.408947</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.317200</td>\n","      <td>6.334692</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>6.215600</td>\n","      <td>6.309314</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>6.240400</td>\n","      <td>6.192551</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>6.175300</td>\n","      <td>6.138880</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>6.109300</td>\n","      <td>6.114434</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.999200</td>\n","      <td>6.050646</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.942900</td>\n","      <td>5.947344</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.891400</td>\n","      <td>5.922942</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.809800</td>\n","      <td>5.858764</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.874800</td>\n","      <td>5.856677</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.725000</td>\n","      <td>5.697091</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.748900</td>\n","      <td>5.705881</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.691100</td>\n","      <td>5.734134</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.617500</td>\n","      <td>5.678579</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.623400</td>\n","      <td>5.607348</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.555700</td>\n","      <td>5.624840</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.584100</td>\n","      <td>5.637661</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.474600</td>\n","      <td>5.607300</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.479700</td>\n","      <td>5.586270</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>5.477300</td>\n","      <td>5.548668</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>5.433000</td>\n","      <td>5.492341</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>5.403600</td>\n","      <td>5.440227</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>5.406800</td>\n","      <td>5.380825</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>5.359700</td>\n","      <td>5.390903</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>5.286600</td>\n","      <td>5.421735</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>5.320000</td>\n","      <td>5.364042</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>5.242800</td>\n","      <td>5.320073</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>5.301000</td>\n","      <td>5.390306</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>5.272900</td>\n","      <td>5.382905</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>5.243900</td>\n","      <td>5.349640</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>5.221100</td>\n","      <td>5.294583</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>5.248600</td>\n","      <td>5.198127</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>5.241700</td>\n","      <td>5.267162</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final\n","Configuration saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/config.json\n","Model weights saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/pytorch_model.bin\n","tokenizing:   0%|          | 0/149798 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 149798/149798 [00:36<00:00, 4085.25it/s]\n","loading configuration file /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"dim\": 512,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dim\": 2048,\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"roberta\",\n","  \"n_heads\": 8,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 100000\n","}\n","\n","loading weights file /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/pytorch_model.bin\n","Loading PyTorch weights from /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/pytorch_model.bin\n","PyTorch checkpoint contains 165,699,488 parameters\n","Loaded 165,003,264 parameters in the TF 2.0 model.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 165593856   input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","__________________________________________________________________________________________________\n","global_average_pooling1d_1 (Glo (None, 768)          0           roberta[0][13]                   \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 14)           10766       global_average_pooling1d_1[0][0] \n","==================================================================================================\n","Total params: 165,604,622\n","Trainable params: 165,604,622\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["1639/1639 [==============================] - 521s 307ms/step - loss: 0.7234 - f1_score: nan - val_loss: 0.6281 - val_f1_score: 0.7940\n","Epoch 2/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.5320 - f1_score: 0.8239 - val_loss: 0.5968 - val_f1_score: 0.8078\n","Epoch 3/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.4371 - f1_score: 0.8565 - val_loss: 0.5793 - val_f1_score: 0.8172\n","Epoch 4/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.3555 - f1_score: 0.8826 - val_loss: 0.6061 - val_f1_score: 0.8153\n","Epoch 5/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.2839 - f1_score: 0.9066 - val_loss: 0.6464 - val_f1_score: 0.8143\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses, pooler_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 1055). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_wordpiece_100k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_wordpiece_100k/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUFpT86YSP_R","executionInfo":{"status":"ok","timestamp":1628092846470,"user_tz":-180,"elapsed":22974,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"0325c296-aa73-4d78-e747-5ac0842c9e7e"},"source":["pred_desc2 = prediction(config_name=\"predict_roberta.yaml\", cat=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tokenizing:   0%|          | 0/676 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676/676 [00:00<00:00, 4369.16it/s]\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"dim\": 512,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dim\": 2048,\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"roberta\",\n","  \"n_heads\": 8,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 100000\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["3/3 [==============================] - 4s 245ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1GnLnW5xzxOv"},"source":["# tst = (\n","#     last_predict_image[last_predict_image.sign_0 != -1]['sign_0']\n","#     .map(MAPPER)\n","#     .map(label_to_typology)\n","# )\n","sample_submission.loc[pred_desc.index, 'typology'] = pred_desc2.values\n","sample_submission.loc[tst.index, 'typology'] = tst.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWwY0WLm31Ky"},"source":["sample_submission.to_csv(SUB_PATH / 'imroberta2.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9BuRh4F_wTz"},"source":["import tensorflow as tf\n","physical_devices = tf.config.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6f786lv48vLz","executionInfo":{"status":"ok","timestamp":1628097985045,"user_tz":-180,"elapsed":1675979,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"9b60c3b9-bbd0-4f65-c592-28690aab9b1d"},"source":["train_description_model(config_name='rob_pretrained.yaml', skip_fitting_tokenizer=True, skip_fitting_lm_model=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                               –ø—Ä–æ—á–∏–µ\n","12                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","13                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","14                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/train_data.csv 15\n"],"name":"stdout"},{"output_type":"stream","text":["loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/7fbac018f5fe478995bca409059883685f3f1325a67b1cb16fe340325af1cea7.018f85b6550237c27386c0ec90a1ff7bdcf74e56a9e2d32131e29c4689192eaa\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/0c7965501fba622370adf334149813060bb231408a6582b155bf623cd1f81d47.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ab4807a64e393da687bd95f59450f96a90d378d830de98889b58383c82a99480.7d8cf5940d60559bc588a922cc36816803bd6aa4db1f2dd48db71df501ac6ea3\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/tokenizer.json from cache at None\n","loading configuration file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c6da12aa84b0056f7fcf2ce40343ab08fd71045914c431fd8bf57709efc5abae.e8f15c5aad2f4653e46ceeba0bb32c02a629d106a902c964bce60523d290ac8f\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","tokenizing:   0%|          | 0/157449 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157449/157449 [01:19<00:00, 1983.22it/s]\n","loading configuration file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c6da12aa84b0056f7fcf2ce40343ab08fd71045914c431fd8bf57709efc5abae.e8f15c5aad2f4653e46ceeba0bb32c02a629d106a902c964bce60523d290ac8f\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"directionality\": \"bidi\",\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e652de99ad5f614a638b468be975a6b8af6466fc9d9285e96bab58e6da989384.b0463ea0c0c83c6c37032f7b1168f52dad0807c3c45c67c2995b9714a4397429\n","Loading PyTorch weights from /root/.cache/huggingface/transformers/e652de99ad5f614a638b468be975a6b8af6466fc9d9285e96bab58e6da989384.b0463ea0c0c83c6c37032f7b1168f52dad0807c3c45c67c2995b9714a4397429\n","PyTorch checkpoint contains 177,853,440 parameters\n","Loaded 177,853,440 parameters in the TF 2.0 model.\n","All PyTorch model weights were used when initializing TFRobertaModel.\n","\n","All the weights of TFRobertaModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_6\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_15 (InputLayer)           [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_16 (InputLayer)           [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 177853440   input_15[0][0]                   \n","                                                                 input_16[0][0]                   \n","__________________________________________________________________________________________________\n","global_average_pooling1d_6 (Glo (None, 768)          0           roberta[0][13]                   \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 15)           11535       global_average_pooling1d_6[0][0] \n","==================================================================================================\n","Total params: 177,864,975\n","Trainable params: 177,864,975\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["1723/1723 [==============================] - 552s 309ms/step - loss: 0.7615 - f1_score: nan - val_loss: 0.6447 - val_f1_score: 0.7904\n","Epoch 2/5\n","1723/1723 [==============================] - 529s 307ms/step - loss: 0.5461 - f1_score: 0.8225 - val_loss: 0.6135 - val_f1_score: 0.8100\n","Epoch 3/5\n","1723/1723 [==============================] - 530s 307ms/step - loss: 0.4513 - f1_score: 0.8522 - val_loss: 0.6254 - val_f1_score: 0.8140\n","Epoch 4/5\n","1723/1723 [==============================] - 530s 308ms/step - loss: 0.3711 - f1_score: 0.8776 - val_loss: 0.6653 - val_f1_score: 0.8093\n","Epoch 5/5\n","1723/1723 [==============================] - 530s 307ms/step - loss: 0.2972 - f1_score: 0.9017 - val_loss: 0.7177 - val_f1_score: 0.8059\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses, pooler_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 1055). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_pretrained_wordpiece_100k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_pretrained_wordpiece_100k/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wXev_i6x_G9-"},"source":[""],"execution_count":null,"outputs":[]}]}