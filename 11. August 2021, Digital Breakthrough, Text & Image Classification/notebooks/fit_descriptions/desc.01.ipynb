{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"desc.01.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2JQuEQqEQ5m0"},"source":["# Fitting Description"]},{"cell_type":"markdown","metadata":{"id":"JSG4Ngd5Q92E"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KDwqlJw0cn7","executionInfo":{"status":"ok","timestamp":1627647681576,"user_tz":-180,"elapsed":4,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"fd5e7c31-18b6-45b1-cb92-006ef8cfa45a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":940},"id":"BZnPDvh_oa1m","executionInfo":{"status":"ok","timestamp":1627647673710,"user_tz":-180,"elapsed":10468,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"2b31df9f-4ebf-41d1-f476-bf1d58d51d4c"},"source":["!pip install transformers\n","!pip install omegaconf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[?25l\r\u001b[K     |▏                               | 10 kB 36.0 MB/s eta 0:00:01\r\u001b[K     |▎                               | 20 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |▍                               | 30 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |▌                               | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 51 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 112 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 122 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 133 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 143 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 153 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 163 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 174 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 184 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 194 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 204 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 215 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 225 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 235 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 245 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 256 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 266 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 276 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 286 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 296 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 307 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 317 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 327 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 337 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 348 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 358 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 368 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 378 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 389 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 399 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 409 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 419 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 430 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 440 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 450 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 460 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 471 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 481 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 491 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 501 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 512 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 522 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 532 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 542 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 552 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 563 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 573 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 583 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 593 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 604 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 614 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 624 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 634 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 645 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 655 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 665 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 675 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 686 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 696 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 706 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 716 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 727 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 737 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 747 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 757 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 768 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 778 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 788 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 798 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 808 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 819 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 829 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 839 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 849 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 860 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 870 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 880 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 890 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 901 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 911 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 921 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 931 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 942 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 952 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 962 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 972 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 983 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 993 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.7 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.8 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.1 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.2 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.4 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.5 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.6 MB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.6 MB 8.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 45.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 57.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 83.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n","Collecting omegaconf\n","  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf) (5.4.1)\n","Collecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 14.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=0edebe322936a84af81b5856156dcb2a595ffca9360e56b825a3dc94f11386c3\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, omegaconf\n","Successfully installed antlr4-python3-runtime-4.8 omegaconf-2.1.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"K5m4OpF7PEbo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627647684665,"user_tz":-180,"elapsed":216,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"1785c30e-0774-440d-b177-5cea2fe4a5c1"},"source":["cd /content/drive/MyDrive/digital_breakthrough/task_3"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9WTDqIM-RAbs"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"tNSlfeQcPODN"},"source":["import sys\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","sys.path.append('.')\n","from definitions import ROOT_DIR"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQ6PnWiUPaTP"},"source":["DATA_PATH = ROOT_DIR / 'data'\n","TRAIN_IMAGES = DATA_PATH / 'images'\n","DOWNLOADED_TRAIN_IMAGES = DATA_PATH / 'downloaded_images'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXCupp9rPcsm"},"source":["train = pd.read_csv(DATA_PATH / 'train.csv')\n","train_url_only = pd.read_csv(DATA_PATH / 'train_url_only.csv')\n","train_url_loaded_images = pd.read_csv(DATA_PATH / 'train_loaded_images.csv')\n","test = pd.read_csv(DATA_PATH / 'test.csv')\n","sample_submission = pd.read_csv(DATA_PATH / 'sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPDB0aK_KoMk"},"source":["from os import listdir\n","train_images = listdir(TRAIN_IMAGES)\n","guid_train_images = [f.split('.')[0] for f in train_images]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RitLsdFDKrzX","executionInfo":{"status":"ok","timestamp":1627635459021,"user_tz":-180,"elapsed":5,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"f53d7d8a-8ed6-4dca-a2eb-1ac5f7b5549d"},"source":["dummy = test[test.guid.isin(guid_train_images)]\n","test_only_description = test[~(test.guid.isin(guid_train_images))]\n","test_only_images = dummy[dummy.description.isna()]\n","test_images_and_description = dummy[~(dummy.description.isna())]\n","print('without image or description:', len(test_only_description[test_only_description.description.isna()]))\n","print('only description:', test_only_description.shape[0])\n","print('only images:', test_only_images.shape[0])\n","print('images and description:', test_images_and_description.shape[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["without image or description: 0\n","only description: 78\n","only images: 547\n","images and description: 598\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z9dAOXBwQz51"},"source":[" ## FitDesc"]},{"cell_type":"markdown","metadata":{"id":"AUq1AtwCRUjC"},"source":["### Models"]},{"cell_type":"code","metadata":{"id":"Fo2Y6He0OEmu"},"source":["import tensorflow as tf\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Input\n","from tensorflow.keras.models import Model\n","from transformers import TFDistilBertModel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhJLXX0zRarr"},"source":["def distilbert_model(input_shape,\n","                     transformer_model,\n","                     output_shape=15,\n","                     output_activation='softmax',\n","                     optimizer='Adam',\n","                     optimizer_params={'lr': 1e-5},\n","                     loss='categorical_crossentropy',\n","                     metrics=None):\n","\n","    input_ids = Input((input_shape,), dtype=tf.int32)\n","    input_mask = Input((input_shape,), dtype=tf.int32)\n","\n","    transformer_encoder = TFDistilBertModel.from_pretrained(\n","        transformer_model,\n","        from_pt=True,\n","        output_hidden_states=True\n","    )\n","    outputs = transformer_encoder.distilbert(input_ids,\n","                                             attention_mask=input_mask)\n","\n","    x = outputs[0]\n","    x = GlobalAveragePooling1D()(x)\n","    output = Dense(output_shape,\n","                   activation=output_activation)(x)\n","\n","    model = Model(inputs=[input_ids, input_mask],\n","                  outputs=output)\n","    model.compile(loss=loss,\n","                  metrics=metrics,\n","                  optimizer=getattr(optimizers, optimizer)(**optimizer_params)\n","                  )\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dZKWnPDR3bp"},"source":["### Preprocess"]},{"cell_type":"code","metadata":{"id":"yVTwvciTR7sY"},"source":["\"\"\"\n","Preprocessing.\n","\"\"\"\n","\n","import numpy as np\n","from tqdm import tqdm\n","from transformers import PreTrainedTokenizerFast\n","\n","\n","def preprocess(texts, tokenizer_path, max_len=32):\n","\n","    input_ids, input_masks = [], []\n","\n","    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n","    tokenizer.mask_token = '[MASK]'\n","    tokenizer.pad_token = \"[PAD]\"\n","    tokenizer.sep_token = \"[SEP]\"\n","    tokenizer.cls_token = \"[CLS]\"\n","    tokenizer.unk_token = \"[UNK]\"\n","\n","    for text in tqdm(texts):\n","        encoded = tokenizer.encode_plus(text,\n","                                        max_length=max_len,\n","                                        pad_to_max_length=True,\n","                                        truncation=True)\n","        input_ids.append(encoded['input_ids'])\n","        input_masks.append(encoded['attention_mask'])\n","\n","    return [np.array(input_ids), np.array(input_masks)]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eOcZ-RvRSD4v"},"source":["### Prepared Data"]},{"cell_type":"markdown","metadata":{"id":"eK2pP1KKZvA4"},"source":["#### Get Full Clean Data"]},{"cell_type":"code","metadata":{"id":"KkVEoBBYXp7b"},"source":["train_url_only['typology'] = train_url_only.typology.replace(\n","    {'предметы прикладного искусства, быта и этнографии ': 'предметы прикладного искусства, быта и этнографии'}\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uExmrXgiSUL_"},"source":["train_labels = train.typology.unique()\n","typology_to_label = dict(zip(sorted(train_labels), range(len(train_labels))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAqCfhVyWBU-"},"source":["train_labels_url = train_url_only[~(train_url_only.typology.isna())].typology.unique()\n","typology_to_label_url = dict(zip(sorted(train_labels_url), range(len(train_labels_url))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKAhgMwnX2SL","executionInfo":{"status":"ok","timestamp":1627635467063,"user_tz":-180,"elapsed":3,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"ada49bdb-9fc9-44b1-ccf7-a2fd36ee1662"},"source":["train_url_only_train_labels = train_url_only[train_url_only.typology.isin(typology_to_label.keys())]\n","print(len(train_url_only_train_labels))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["197117\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJWN_85XYjUf","executionInfo":{"status":"ok","timestamp":1627635467760,"user_tz":-180,"elapsed":4,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"72581503-94b3-402d-cd00-be93b59d0fab"},"source":["train['url'] = 1\n","full_train = pd.concat((train, train_url_only_train_labels), axis=0)\n","full_train = full_train[~(full_train.typology.isna())]\n","full_train = full_train[~(full_train.description.isna())]\n","full_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(201852, 4)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"jHEzvTp4Z0CI"},"source":["#### Prepare labels"]},{"cell_type":"markdown","metadata":{"id":"6rgyzIt9bF4L"},"source":["##### Save Item Names"]},{"cell_type":"code","metadata":{"id":"fJomhdvVZ55Q"},"source":["item_names = train.description.drop_duplicates()\n","item_names = item_names.map(lambda x: x + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONPV8v4Ia0xX"},"source":["with open('./data/item_name.txt', 'w') as f:\n","    f.writelines(item_names.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTOC6gKcbKkq"},"source":["##### Save Full Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNh-rfpYbNcw","executionInfo":{"status":"ok","timestamp":1627635469091,"user_tz":-180,"elapsed":5,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"10edb16c-33be-4325-ae43-0cae3d0147b8"},"source":["save_train = full_train.drop_duplicates('description')\n","save_train = save_train[save_train.description != '']\n","print(save_train.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(157449, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"soruBzUFbmR5"},"source":["save_train.to_csv('./data/train_data.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2O7MbVubN97"},"source":["##### Prepare list of categories"]},{"cell_type":"code","metadata":{"id":"-qkKwdSWbQKX"},"source":["categories = sorted(save_train.typology.unique())\n","categories = pd.Series(categories, name='category')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3hi2WCpbvjO"},"source":["categories.to_csv('./data/categories.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o9RDs6rEcC9h"},"source":["### Train Tokenizers"]},{"cell_type":"code","metadata":{"id":"HXmzIPh2cP1Y"},"source":["import os\n","import pandas as pd\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE, WordPiece, Unigram\n","from tokenizers.normalizers import Lowercase\n","from tokenizers.pre_tokenizers import Whitespace, Digits, Sequence\n","from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMBH-3fIiBLY"},"source":["tokenizers = {\n","    1: {\n","        'tokenizer': Tokenizer(WordPiece(unk_token=\"[UNK]\")),\n","        'pre_tokenizer': Sequence([Whitespace(), Digits()]),\n","        'normalizer': Lowercase(),\n","        'trainer': WordPieceTrainer(\n","            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n","            vocab_size=70000),\n","        'name': 'wordpiece_70k.json'\n","        },\n","    2: {\n","        'tokenizer': Tokenizer(BPE(unk_token=\"[UNK]\")),\n","        'pre_tokenizer': Sequence([Whitespace(), Digits()]),\n","        'normalizer': Lowercase(),\n","        'trainer': BpeTrainer(\n","            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n","            vocab_size=60000),\n","        'name': 'bpe_60k.json'\n","        },\n","    3: {\n","        'tokenizer': Tokenizer(Unigram()),\n","        'pre_tokenizer': Sequence([Whitespace(), Digits()]),\n","        'normalizer': Lowercase(),\n","        'trainer': UnigramTrainer(\n","            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n","            vocab_size=50000),\n","        'name': 'unigram_50k.json'\n","        }\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iF759FycdN1o"},"source":["class TokenizerFabrica(object):\n","\n","    def __init__(self, \n","                 tokenizer,\n","                 pre_tokenizer,\n","                 normalizer,\n","                 trainer,\n","                 name):\n","        self.tokenizer = tokenizer\n","        self.tokenizer.pre_tokenizer = pre_tokenizer\n","        self.tokenizer.normalizer = normalizer\n","        self.trainer = trainer\n","        self.name = name\n","        self.fitted = False\n","    \n","    def fit(self, item_names):\n","        self.tokenizer.train_from_iterator(item_names, \n","                                           self.trainer)\n","        self.fitted = True\n","\n","    def save_model(self, output):\n","        path = str(output / self.name)\n","        if self.fitted:\n","            self.tokenizer.save(path)\n","            return path\n","        else:\n","            raise ValueError('Fit tokenizer before saving')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUmX8rEskgtS"},"source":["with open('./data/item_name.txt', 'r') as f:\n","    items = f.readlines()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bvcNAnYlMaI"},"source":["MODELS_PATH = ROOT_DIR / 'models'\n","TOKENIZERS_PATH = MODELS_PATH / 'tokenizers'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDLAjaM1tcB8"},"source":["for config in tokenizers.values():\n","    token = TokenizerFabrica(**config)\n","    token.fit(items)\n","    path = token.save_model(TOKENIZERS_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GN9SQssLnbEv"},"source":["### Train Language Model"]},{"cell_type":"code","metadata":{"id":"zLMbvC5ZomRQ"},"source":["import argparse\n","import os\n","\n","from omegaconf import OmegaConf\n","from transformers import (\n","    DataCollatorForLanguageModeling, \n","    DistilBertConfig,\n","    DistilBertForMaskedLM, \n","    LineByLineTextDataset, \n","    PreTrainedTokenizerFast, \n","    Trainer, \n","    TrainingArguments\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lk_fbzlbpECM"},"source":["os.environ['WANDB_DISABLED'] = 'true'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"42wjAe26p8Vy"},"source":["class LanguageModel(object):\n","\n","    def __init__(self, config):\n","        self.tokenizer = PreTrainedTokenizerFast(\n","            tokenizer_file=config.tokenizer_path\n","            )\n","        self.tokenizer.mask_token = '[MASK]'\n","        self.tokenizer.pad_token = \"[PAD]\"\n","        self.tokenizer.sep_token = \"[SEP]\"\n","        self.tokenizer.cls_token = \"[CLS]\"\n","        self.tokenizer.unk_token = \"[UNK]\"\n","        self.distilbert_config = DistilBertConfig(\n","            vocab_size=config.vocab_size,\n","            n_heads=8, \n","            dim=512, \n","            hidden_dim=2048\n","            )\n","        self.model = DistilBertForMaskedLM(self.distilbert_config)\n","        self.dataset = LineByLineTextDataset(\n","            tokenizer=self.tokenizer,\n","            file_path=config.item_names_path,\n","            block_size=64\n","            )\n","        self.data_collator = DataCollatorForLanguageModeling(\n","            tokenizer=self.tokenizer,\n","            mlm=True,\n","            mlm_probability=config.mlm_probability\n","            )\n","        self.training_args = TrainingArguments(\n","            output_dir=config.output_path,\n","            overwrite_output_dir=True,\n","            num_train_epochs=config.num_train_epochs,\n","            learning_rate=config.learning_rate,\n","            per_device_train_batch_size=config.batch_size,\n","            save_steps=300000,\n","            save_total_limit=1\n","            )\n","        self.trainer = Trainer(\n","            model=self.model,\n","            args=self.training_args,\n","            data_collator=self.data_collator,\n","            train_dataset=self.dataset,\n","            eval_dataset=None\n","            )\n","        self.fitted = False\n","        self.config = config\n","    \n","    def fit(self):\n","        self.trainer.train()\n","        self.fitted = True\n","\n","    def save_model(self):\n","        if self.fitted:\n","            self.trainer.save_model(\n","                os.path.join(self.config.output_path, 'final')\n","                )\n","        else:\n","            raise ValueError('Fit tokenizer before saving')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wEJIHWE0thx4","executionInfo":{"status":"ok","timestamp":1627637506640,"user_tz":-180,"elapsed":367016,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"98925916-fcf2-4c57-91fa-30f769862187"},"source":["for i in range(1, 4):\n","    config = OmegaConf.load(f'./src/configs/train_lm{i}.yaml')\n","    print(OmegaConf.to_yaml(config))\n","    lm = LanguageModel(config)\n","    lm.fit()\n","    lm.save_model()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tokenizer_path: ./models/tokenizers/wordpiece_70k.json\n","vocab_size: 70000\n","mlm_probability: 0.3\n","output_path: ./models/lm_models/distilbert_lm_wordpiece_70k\n","num_train_epochs: 8\n","learning_rate: 5.0e-05\n","batch_size: 32\n","item_names_path: ./data/item_name.txt\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running training *****\n","  Num examples = 4402\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1104\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1104' max='1104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1104/1104 01:55, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>7.593400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>6.743200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./models/lm_models/distilbert_lm_wordpiece_70k/final\n","Configuration saved in ./models/lm_models/distilbert_lm_wordpiece_70k/final/config.json\n","Model weights saved in ./models/lm_models/distilbert_lm_wordpiece_70k/final/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["tokenizer_path: ./models/tokenizers/bpe_60k.json\n","vocab_size: 60000\n","mlm_probability: 0.2\n","output_path: ./models/lm_models/distilbert_lm_bpe_60k\n","num_train_epochs: 10\n","learning_rate: 5.0e-05\n","batch_size: 32\n","item_names_path: ./data/item_name.txt\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Creating features from dataset file at ./data/item_name.txt\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running training *****\n","  Num examples = 4402\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1380\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1380/1380 02:11, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>7.631800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>6.785000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./models/lm_models/distilbert_lm_bpe_60k/final\n","Configuration saved in ./models/lm_models/distilbert_lm_bpe_60k/final/config.json\n","Model weights saved in ./models/lm_models/distilbert_lm_bpe_60k/final/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["tokenizer_path: ./models/tokenizers/unigram_50k.json\n","vocab_size: 50000\n","mlm_probability: 0.2\n","output_path: ./models/lm_models/distilbert_lm_unigram_50k\n","num_train_epochs: 7\n","learning_rate: 5.0e-05\n","batch_size: 32\n","item_names_path: ./data/item_name.txt\n","\n"],"name":"stdout"},{"output_type":"stream","text":["Creating features from dataset file at ./data/item_name.txt\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running training *****\n","  Num examples = 4402\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 966\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='966' max='966' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [966/966 01:43, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>6.894400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./models/lm_models/distilbert_lm_unigram_50k/final\n","Configuration saved in ./models/lm_models/distilbert_lm_unigram_50k/final/config.json\n","Model weights saved in ./models/lm_models/distilbert_lm_unigram_50k/final/pytorch_model.bin\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rRW6isONsip8"},"source":["### Train All Data"]},{"cell_type":"code","metadata":{"id":"HjgekJ3tssXO"},"source":["import argparse\n","import pandas as pd\n","import tensorflow as tf\n","from omegaconf import OmegaConf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltrvqbASs8T-"},"source":["tf.config.experimental.set_memory_growth(\n","    device=tf.config.experimental.get_visible_devices('GPU')[0],\n","    enable=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDA00OwRs17O"},"source":["data = pd.read_csv('./data/train_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ZSUIfGwEnccc","executionInfo":{"status":"error","timestamp":1627640179591,"user_tz":-180,"elapsed":2358510,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"c61b93b9-8d3a-4c0c-e6a3-e940c9e5cedf"},"source":["for i in range(1, 4):\n","    config = OmegaConf.load(f'./src/configs/train{i}.yaml')\n","    print(OmegaConf.to_yaml(config))\n","    X = preprocess(data.description, **config.preprocess)\n","    y = pd.get_dummies(data.typology)\n","    model = distilbert_model(**config.model)\n","    print(model.summary())\n","    model.fit(X, y, verbose=1, **config.train)\n","    model.save(config.output_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/157449 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 986/157449 [00:00<00:15, 9855.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["output_path: ./models/model/distilbert_wordpiece_70k\n","preprocess:\n","  tokenizer_path: ./models/tokenizers/wordpiece_70k.json\n","  max_len: 32\n","model:\n","  input_shape: 32\n","  transformer_model: ./models/lm_models/distilbert_lm_wordpiece_70k/final/\n","train:\n","  epochs: 5\n","  batch_size: 64\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 157449/157449 [00:18<00:00, 8453.00it/s]\n","loading configuration file ./models/lm_models/distilbert_lm_wordpiece_70k/final/config.json\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 512,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 8,\n","  \"n_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"vocab_size\": 70000\n","}\n","\n","loading weights file ./models/lm_models/distilbert_lm_wordpiece_70k/final/pytorch_model.bin\n","Loading PyTorch weights from /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_lm_wordpiece_70k/final/pytorch_model.bin\n","PyTorch checkpoint contains 91,191,152 parameters\n","Loaded 55,017,472 parameters in the TF 2.0 model.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","distilbert (TFDistilBertMainLay TFBaseModelOutput(la 55017472    input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","__________________________________________________________________________________________________\n","global_average_pooling1d_1 (Glo (None, 512)          0           distilbert[0][7]                 \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 15)           7695        global_average_pooling1d_1[0][0] \n","==================================================================================================\n","Total params: 55,025,167\n","Trainable params: 55,025,167\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","2461/2461 [==============================] - 204s 80ms/step - loss: 1.1153\n","Epoch 2/5\n","2461/2461 [==============================] - 197s 80ms/step - loss: 0.7729\n","Epoch 3/5\n","2461/2461 [==============================] - 197s 80ms/step - loss: 0.6689\n","Epoch 4/5\n","2461/2461 [==============================] - 198s 81ms/step - loss: 0.6015\n","Epoch 5/5\n","2461/2461 [==============================] - 198s 80ms/step - loss: 0.5488\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63e15aa990>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63de8543d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63df4a5610>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63e219d810>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63dbc18650>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63e3a95f10>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses, add_1_layer_call_fn while saving (showing 5 of 415). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ./models/model/distilbert_wordpiece_70k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ./models/model/distilbert_wordpiece_70k/assets\n","  0%|          | 0/157449 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 890/157449 [00:00<00:17, 8897.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["output_path: ./models/model/distilbert_bpe_60k\n","preprocess:\n","  tokenizer_path: ./models/tokenizers/bpe_60k.json\n","  max_len: 32\n","model:\n","  input_shape: 32\n","  transformer_model: ./models/lm_models/distilbert_lm_bpe_60k/final/\n","train:\n","  epochs: 6\n","  batch_size: 64\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 157449/157449 [00:21<00:00, 7444.00it/s]\n","loading configuration file ./models/lm_models/distilbert_lm_bpe_60k/final/config.json\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 512,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 8,\n","  \"n_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"vocab_size\": 60000\n","}\n","\n","loading weights file ./models/lm_models/distilbert_lm_bpe_60k/final/pytorch_model.bin\n","Loading PyTorch weights from /content/drive/My Drive/digital_breakthrough/task_3/models/lm_models/distilbert_lm_bpe_60k/final/pytorch_model.bin\n","PyTorch checkpoint contains 80,941,152 parameters\n","Loaded 49,897,472 parameters in the TF 2.0 model.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_6 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","distilbert (TFDistilBertMainLay TFBaseModelOutput(la 49897472    input_5[0][0]                    \n","                                                                 input_6[0][0]                    \n","__________________________________________________________________________________________________\n","global_average_pooling1d_2 (Glo (None, 512)          0           distilbert[0][7]                 \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 15)           7695        global_average_pooling1d_2[0][0] \n","==================================================================================================\n","Total params: 49,905,167\n","Trainable params: 49,905,167\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/6\n","2461/2461 [==============================] - 202s 80ms/step - loss: 1.1101\n","Epoch 2/6\n","2461/2461 [==============================] - 196s 80ms/step - loss: 0.7772\n","Epoch 3/6\n","2461/2461 [==============================] - 196s 80ms/step - loss: 0.6769\n","Epoch 4/6\n","2461/2461 [==============================] - 196s 79ms/step - loss: 0.6112\n","Epoch 5/6\n","2461/2461 [==============================] - 196s 80ms/step - loss: 0.5598\n","Epoch 6/6\n","2461/2461 [==============================] - 196s 80ms/step - loss: 0.5184\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63de2a4690>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63de2a4690>, because it is not built.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63dcef00d0>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63dcef00d0>, because it is not built.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63e218fc10>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63e218fc10>, because it is not built.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63db2f8a50>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63db2f8a50>, because it is not built.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63df4b9350>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63df4b9350>, because it is not built.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63de850d90>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f63de850d90>, because it is not built.\n","WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses, add_2_layer_call_fn while saving (showing 5 of 415). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ./models/model/distilbert_bpe_60k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ./models/model/distilbert_bpe_60k/assets\n","  0%|          | 0/157449 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0%|          | 631/157449 [00:00<00:24, 6301.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["output_path: ./models/model/distilbert_unigram_50k\n","preprocess:\n","  tokenizer_path: ./models/tokenizers/unigram_50k.json\n","  max_len: 32\n","model:\n","  input_shape: 32\n","  transformer_model: ./models/lm_models/distilbert_lm_unigram_50k/final/\n","train:\n","  epochs: 5\n","  batch_size: 64\n","\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 4059/157449 [00:00<00:25, 6108.02it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-65cddc080859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./src/configs/train{i}.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypology\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-4be62f810a25>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(texts, tokenizer_path, max_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                         \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                         truncation=True)\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0minput_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2477\u001b[0m         )\n\u001b[1;32m   2478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         )\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         )\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Encountered an unknown token but `unk_id` is missing"]}]},{"cell_type":"markdown","metadata":{"id":"RQ80fEWE5Pz5"},"source":["### Prediction"]},{"cell_type":"code","metadata":{"id":"UsfF_9ZN63VG"},"source":["from tensorflow.keras.models import load_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yF7sYUNwxRRO"},"source":["test_ = test[~(test.description.isna())]\n","categories = pd.read_csv('./data/categories.csv')['category'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFz1A-uf3XtL"},"source":["tokenizers = [str(TOKENIZERS_PATH / name) for name in ['wordpiece_70k.json', 'bpe_60k.json']]\n","models = ['./models/model/distilbert_wordpiece_70k', './models/model/distilbert_bpe_60k']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0QVDTF64a6w","executionInfo":{"status":"ok","timestamp":1627640286056,"user_tz":-180,"elapsed":17376,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"2438b3f9-d7ce-4edd-9d4b-e8361a58ffab"},"source":["probs = []\n","item_name = test_.description\n","for model, tokenizer in zip(models, tokenizers):\n","    input = preprocess(item_name, tokenizer_path=tokenizer)\n","    mdl = load_model(model)\n","    proba = mdl.predict(input, batch_size=256, verbose=True)\n","    probs.append(proba)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/676 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 676/676 [00:00<00:00, 8638.06it/s]\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"./models/lm_models/distilbert_lm_wordpiece_70k/final/\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 512,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 8,\n","  \"n_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"vocab_size\": 70000\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["3/3 [==============================] - 1s 75ms/step\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 676/676 [00:00<00:00, 7958.38it/s]\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"./models/lm_models/distilbert_lm_bpe_60k/final/\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 512,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 8,\n","  \"n_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"vocab_size\": 60000\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["3/3 [==============================] - 1s 71ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6-RvM6zS61bM"},"source":["pb = probs[0] + probs[1]\n","df_pb = pd.DataFrame(pb, columns=categories)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhGmTb767LJU"},"source":["pred = df_pb.idxmax(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5HJueDh9CbR","executionInfo":{"status":"ok","timestamp":1627640859707,"user_tz":-180,"elapsed":795,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"","userId":"08072078113294094856"}},"outputId":"b370bc35-a113-46e0-9fc1-4e9d920e4902"},"source":["len(test) - len(test_)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["547"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"_VNeMZWj7Z5F"},"source":["sample_submission['typology'] = 'прочие'\n","sample_submission.loc[test_.index, 'typology'] = pred.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtaeZ9KwXObh"},"source":["sam = pd.read_csv('./sub/distilbert.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiY4IZqr7haV"},"source":["sample_submission.to_csv('./sub/distilbert.csv', index=False)"],"execution_count":null,"outputs":[]}]}