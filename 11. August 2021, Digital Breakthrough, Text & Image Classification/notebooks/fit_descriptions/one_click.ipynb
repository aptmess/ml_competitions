{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"one_click.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2JQuEQqEQ5m0"},"source":["# Fitting Description"]},{"cell_type":"markdown","metadata":{"id":"JSG4Ngd5Q92E"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KDwqlJw0cn7","executionInfo":{"status":"ok","timestamp":1628075942581,"user_tz":-180,"elapsed":413,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"39a0ca64-4d80-425d-9bdd-d184df617989"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K5m4OpF7PEbo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628075942581,"user_tz":-180,"elapsed":7,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"cfdd6ac0-11b6-4030-d9e6-8f34203bba78"},"source":["cd /content/drive/MyDrive/digital_breakthrough/task_3"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g57naKGtkvGZ"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFPM4lPrk7-Q","executionInfo":{"status":"ok","timestamp":1628074345670,"user_tz":-180,"elapsed":6,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"8d6ac981-1eee-432c-82a3-66e42869346a"},"source":["cd /content/drive/MyDrive/digital_breakthrough/task_3"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/digital_breakthrough/task_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OJCtJQLslRIO"},"source":["import logging\n","from src.predict import prediction\n","from src.train import train_description_model\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","log = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EXExcFfnlVzH","executionInfo":{"status":"ok","timestamp":1628080887639,"user_tz":-180,"elapsed":997695,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"62ff1bb1-4b98-43be-bfe6-9e3de2403dd7"},"source":["train_description_model(config_name='exp_v1.yaml')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v3\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                               –ø—Ä–æ—á–∏–µ\n","12                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","13                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","14                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v3/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v3/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v3/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v3/train_data.csv 15\n"],"name":"stdout"},{"output_type":"stream","text":["Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 136991\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 21405\n"],"name":"stderr"},{"output_type":"stream","text":["<transformers.data.datasets.language_modeling.LineByLineTextDataset object at 0x7f73c6969610>\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='21001' max='21405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21001/21405 1:05:19 < 01:15, 5.36 it/s, Epoch 4.91/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.157700</td>\n","      <td>7.597106</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.459100</td>\n","      <td>7.330104</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.254500</td>\n","      <td>7.107972</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.084900</td>\n","      <td>6.960293</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>6.935300</td>\n","      <td>6.790084</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>6.783400</td>\n","      <td>6.667325</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>6.664300</td>\n","      <td>6.556479</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>6.540200</td>\n","      <td>6.403884</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.427800</td>\n","      <td>6.329161</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.278700</td>\n","      <td>6.216409</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.203900</td>\n","      <td>6.097263</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.136100</td>\n","      <td>6.036373</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.079800</td>\n","      <td>5.971723</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.000500</td>\n","      <td>5.896763</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>5.913600</td>\n","      <td>5.864035</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>5.885800</td>\n","      <td>5.794699</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>5.850000</td>\n","      <td>5.743946</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>5.707100</td>\n","      <td>5.711656</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.654900</td>\n","      <td>5.625530</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.629700</td>\n","      <td>5.586357</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.648900</td>\n","      <td>5.543045</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.572600</td>\n","      <td>5.531494</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.546800</td>\n","      <td>5.480374</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.497800</td>\n","      <td>5.442695</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.466600</td>\n","      <td>5.418900</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.476800</td>\n","      <td>5.376527</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.381700</td>\n","      <td>5.370348</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.382000</td>\n","      <td>5.341341</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.322300</td>\n","      <td>5.314736</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.351900</td>\n","      <td>5.291893</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.292500</td>\n","      <td>5.294629</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.314800</td>\n","      <td>5.279379</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>5.248500</td>\n","      <td>5.245683</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>5.249400</td>\n","      <td>5.225261</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>5.225900</td>\n","      <td>5.188625</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>5.172600</td>\n","      <td>5.207583</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>5.220300</td>\n","      <td>5.157412</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>5.156700</td>\n","      <td>5.189649</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>5.201500</td>\n","      <td>5.176063</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>5.209100</td>\n","      <td>5.151777</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>5.167400</td>\n","      <td>5.151129</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='163' max='3028' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 163/3028 00:01 < 00:31, 91.41 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 24217\n","  Batch size = 8\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='21405' max='21405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21405/21405 1:06:41, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.157700</td>\n","      <td>7.597106</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.459100</td>\n","      <td>7.330104</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.254500</td>\n","      <td>7.107972</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.084900</td>\n","      <td>6.960293</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>6.935300</td>\n","      <td>6.790084</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>6.783400</td>\n","      <td>6.667325</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>6.664300</td>\n","      <td>6.556479</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>6.540200</td>\n","      <td>6.403884</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.427800</td>\n","      <td>6.329161</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.278700</td>\n","      <td>6.216409</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.203900</td>\n","      <td>6.097263</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.136100</td>\n","      <td>6.036373</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.079800</td>\n","      <td>5.971723</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.000500</td>\n","      <td>5.896763</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>5.913600</td>\n","      <td>5.864035</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>5.885800</td>\n","      <td>5.794699</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>5.850000</td>\n","      <td>5.743946</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>5.707100</td>\n","      <td>5.711656</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.654900</td>\n","      <td>5.625530</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.629700</td>\n","      <td>5.586357</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.648900</td>\n","      <td>5.543045</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.572600</td>\n","      <td>5.531494</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.546800</td>\n","      <td>5.480374</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.497800</td>\n","      <td>5.442695</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.466600</td>\n","      <td>5.418900</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.476800</td>\n","      <td>5.376527</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.381700</td>\n","      <td>5.370348</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.382000</td>\n","      <td>5.341341</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.322300</td>\n","      <td>5.314736</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.351900</td>\n","      <td>5.291893</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.292500</td>\n","      <td>5.294629</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.314800</td>\n","      <td>5.279379</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>5.248500</td>\n","      <td>5.245683</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>5.249400</td>\n","      <td>5.225261</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>5.225900</td>\n","      <td>5.188625</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>5.172600</td>\n","      <td>5.207583</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>5.220300</td>\n","      <td>5.157412</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>5.156700</td>\n","      <td>5.189649</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>5.201500</td>\n","      <td>5.176063</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>5.209100</td>\n","      <td>5.151777</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>5.167400</td>\n","      <td>5.151129</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>5.145200</td>\n","      <td>5.154363</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_80k/final\n","Configuration saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_80k/final/config.json\n","Model weights saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_80k/final/pytorch_model.bin\n","tokenizing:   0%|          | 0/157449 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157449/157449 [00:38<00:00, 4081.98it/s]\n","loading configuration file /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_80k/final/config.json\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 512,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 8,\n","  \"n_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"vocab_size\": 80000\n","}\n","\n","loading weights file /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_80k/final/pytorch_model.bin\n","Loading PyTorch weights from /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_80k/final/pytorch_model.bin\n","PyTorch checkpoint contains 101,441,152 parameters\n","Loaded 60,137,472 parameters in the TF 2.0 model.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","distilbert (TFDistilBertMainLay TFBaseModelOutput(la 60137472    input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 512)          0           distilbert[0][7]                 \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 15)           7695        global_average_pooling1d[0][0]   \n","==================================================================================================\n","Total params: 60,145,167\n","Trainable params: 60,145,167\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","1723/1723 [==============================] - 179s 99ms/step - loss: 0.9249 - f1_score: nan - val_loss: 0.7905 - val_f1_score: 0.7379\n","Epoch 2/5\n","1723/1723 [==============================] - 167s 97ms/step - loss: 0.7088 - f1_score: 0.7682 - val_loss: 0.7291 - val_f1_score: 0.7670\n","Epoch 3/5\n","1723/1723 [==============================] - 167s 97ms/step - loss: 0.6127 - f1_score: 0.8004 - val_loss: 0.7181 - val_f1_score: 0.7759\n","Epoch 4/5\n","1723/1723 [==============================] - 167s 97ms/step - loss: 0.5298 - f1_score: 0.8277 - val_loss: 0.7178 - val_f1_score: 0.7796\n","Epoch 5/5\n","1723/1723 [==============================] - 167s 97ms/step - loss: 0.4520 - f1_score: 0.8538 - val_loss: 0.7497 - val_f1_score: 0.7796\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f72be2ee7d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f72be2f4310>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f72b89c8b90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f72b89ea510>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f72b9042e90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f72b9060810>, because it is not built.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/distilbert_wordpiece_80k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/distilbert_wordpiece_80k/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Yl8lgBavld9v"},"source":["from src.predict import prediction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYeWI0quMfSF","executionInfo":{"status":"ok","timestamp":1628081147212,"user_tz":-180,"elapsed":12004,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"82657272-03cd-48c6-bcde-29b634491196"},"source":["pred_desc = prediction(config_name=\"predict_bert.yaml\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tokenizing:   0%|          | 0/676 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676/676 [00:00<00:00, 4307.44it/s]\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/distilbert_wordpiece_80k/final\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 512,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 8,\n","  \"n_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"vocab_size\": 80000\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["3/3 [==============================] - 2s 73ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9PAEuXi9Mtzd"},"source":["import pandas as pd\n","\n","from definitions import DATA_PATH, SUB_PATH\n","\n","\n","last_predict_image = pd.read_csv(SUB_PATH / 'predictv2.csv')\n","last_predict_all = pd.read_csv(SUB_PATH / 'imbert.csv')\n","\n","train = pd.read_csv(DATA_PATH / 'train.csv')\n","\n","sample_submission = pd.read_csv(DATA_PATH / 'sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDGTj0NiNvsL"},"source":["train_labels = train.typology.unique()\n","typology_to_label = dict(\n","    zip(\n","        sorted(train_labels),\n","        range(\n","            len(train_labels)\n","        )\n","    )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCQ50EIWNxzD"},"source":["label_to_typology = {v: k for k, v in typology_to_label.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Be1WbCRtOJnD"},"source":["MAPPER = {0: 0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, \n","          7:7, 8:8, 9:9, 10:10, 11:12, 12:13, 13:14,\n","          -1: -1\n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0EfhOb5M7Nt"},"source":["tst = (\n","    last_predict_image[last_predict_image.sign_0 != -1]['sign_0']\n","    .map(MAPPER)\n","    .map(label_to_typology)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZrmD0rjOXVq","executionInfo":{"status":"ok","timestamp":1628081640027,"user_tz":-180,"elapsed":207,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"2d9e33de-3a38-4b34-90b4-96aa5195c595"},"source":["(last_predict_all.loc[tst.index, 'typology'] == tst).all()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"5b5QpTaxOYPi"},"source":["sample_submission.loc[pred_desc.index, 'typology'] = pred_desc.values\n","sample_submission.loc[tst.index, 'typology'] = tst.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJn8Ca5WO9IR"},"source":["sample_submission.to_csv(SUB_PATH / 'IDBert.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2b2njVFxSBtD"},"source":["### ROBERTA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8YCS5jLcSCTh","executionInfo":{"status":"ok","timestamp":1628092653802,"user_tz":-180,"elapsed":4793853,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"23f04047-6c39-418e-ab64-271bbeda4ebb"},"source":["train_description_model(config_name='exp_v2:roberta.yaml')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","12                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","13                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/train_data.csv 14\n"],"name":"stdout"},{"output_type":"stream","text":["using `logging_steps` to initialize `eval_steps` to 500\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Creating features from dataset file at /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_train.txt\n","Creating features from dataset file at /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v4/item_name_valid.txt\n","***** Running training *****\n","  Num examples = 147319\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23020\n"],"name":"stderr"},{"output_type":"stream","text":["<transformers.data.datasets.language_modeling.LineByLineTextDataset object at 0x7f72c33b1210>\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='16326' max='23020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [16326/23020 1:27:08 < 35:44, 3.12 it/s, Epoch 3.55/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.313100</td>\n","      <td>7.916295</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.845800</td>\n","      <td>7.702612</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.639700</td>\n","      <td>7.642996</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.512000</td>\n","      <td>7.499543</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>7.383200</td>\n","      <td>7.377310</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>7.246600</td>\n","      <td>7.264523</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>7.107100</td>\n","      <td>7.073224</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>7.001700</td>\n","      <td>6.913572</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.875100</td>\n","      <td>6.713515</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.704000</td>\n","      <td>6.724208</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.601700</td>\n","      <td>6.623839</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.589900</td>\n","      <td>6.480356</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.440400</td>\n","      <td>6.408947</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.317200</td>\n","      <td>6.334692</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>6.215600</td>\n","      <td>6.309314</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>6.240400</td>\n","      <td>6.192551</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>6.175300</td>\n","      <td>6.138880</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>6.109300</td>\n","      <td>6.114434</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.999200</td>\n","      <td>6.050646</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.942900</td>\n","      <td>5.947344</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.891400</td>\n","      <td>5.922942</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.809800</td>\n","      <td>5.858764</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.874800</td>\n","      <td>5.856677</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.725000</td>\n","      <td>5.697091</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.748900</td>\n","      <td>5.705881</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.691100</td>\n","      <td>5.734134</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.617500</td>\n","      <td>5.678579</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.623400</td>\n","      <td>5.607348</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.555700</td>\n","      <td>5.624840</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.584100</td>\n","      <td>5.637661</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.474600</td>\n","      <td>5.607300</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.479700</td>\n","      <td>5.586270</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='23020' max='23020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23020/23020 2:03:30, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>8.313100</td>\n","      <td>7.916295</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>7.845800</td>\n","      <td>7.702612</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>7.639700</td>\n","      <td>7.642996</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>7.512000</td>\n","      <td>7.499543</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>7.383200</td>\n","      <td>7.377310</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>7.246600</td>\n","      <td>7.264523</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>7.107100</td>\n","      <td>7.073224</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>7.001700</td>\n","      <td>6.913572</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.875100</td>\n","      <td>6.713515</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.704000</td>\n","      <td>6.724208</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.601700</td>\n","      <td>6.623839</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.589900</td>\n","      <td>6.480356</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.440400</td>\n","      <td>6.408947</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.317200</td>\n","      <td>6.334692</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>6.215600</td>\n","      <td>6.309314</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>6.240400</td>\n","      <td>6.192551</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>6.175300</td>\n","      <td>6.138880</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>6.109300</td>\n","      <td>6.114434</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>5.999200</td>\n","      <td>6.050646</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>5.942900</td>\n","      <td>5.947344</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>5.891400</td>\n","      <td>5.922942</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>5.809800</td>\n","      <td>5.858764</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>5.874800</td>\n","      <td>5.856677</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>5.725000</td>\n","      <td>5.697091</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>5.748900</td>\n","      <td>5.705881</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>5.691100</td>\n","      <td>5.734134</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>5.617500</td>\n","      <td>5.678579</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>5.623400</td>\n","      <td>5.607348</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>5.555700</td>\n","      <td>5.624840</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>5.584100</td>\n","      <td>5.637661</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>5.474600</td>\n","      <td>5.607300</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>5.479700</td>\n","      <td>5.586270</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>5.477300</td>\n","      <td>5.548668</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>5.433000</td>\n","      <td>5.492341</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>5.403600</td>\n","      <td>5.440227</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>5.406800</td>\n","      <td>5.380825</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>5.359700</td>\n","      <td>5.390903</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>5.286600</td>\n","      <td>5.421735</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>5.320000</td>\n","      <td>5.364042</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>5.242800</td>\n","      <td>5.320073</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>5.301000</td>\n","      <td>5.390306</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>5.272900</td>\n","      <td>5.382905</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>5.243900</td>\n","      <td>5.349640</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>5.221100</td>\n","      <td>5.294583</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>5.248600</td>\n","      <td>5.198127</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>5.241700</td>\n","      <td>5.267162</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 6138\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final\n","Configuration saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/config.json\n","Model weights saved in /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/pytorch_model.bin\n","tokenizing:   0%|          | 0/149798 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 149798/149798 [00:36<00:00, 4085.25it/s]\n","loading configuration file /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"dim\": 512,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dim\": 2048,\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"roberta\",\n","  \"n_heads\": 8,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 100000\n","}\n","\n","loading weights file /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/pytorch_model.bin\n","Loading PyTorch weights from /content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final/pytorch_model.bin\n","PyTorch checkpoint contains 165,699,488 parameters\n","Loaded 165,003,264 parameters in the TF 2.0 model.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 165593856   input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","__________________________________________________________________________________________________\n","global_average_pooling1d_1 (Glo (None, 768)          0           roberta[0][13]                   \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 14)           10766       global_average_pooling1d_1[0][0] \n","==================================================================================================\n","Total params: 165,604,622\n","Trainable params: 165,604,622\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["1639/1639 [==============================] - 521s 307ms/step - loss: 0.7234 - f1_score: nan - val_loss: 0.6281 - val_f1_score: 0.7940\n","Epoch 2/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.5320 - f1_score: 0.8239 - val_loss: 0.5968 - val_f1_score: 0.8078\n","Epoch 3/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.4371 - f1_score: 0.8565 - val_loss: 0.5793 - val_f1_score: 0.8172\n","Epoch 4/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.3555 - f1_score: 0.8826 - val_loss: 0.6061 - val_f1_score: 0.8153\n","Epoch 5/5\n","1639/1639 [==============================] - 500s 305ms/step - loss: 0.2839 - f1_score: 0.9066 - val_loss: 0.6464 - val_f1_score: 0.8143\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses, pooler_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 1055). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_wordpiece_100k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_wordpiece_100k/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUFpT86YSP_R","executionInfo":{"status":"ok","timestamp":1628092846470,"user_tz":-180,"elapsed":22974,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"0325c296-aa73-4d78-e747-5ac0842c9e7e"},"source":["pred_desc2 = prediction(config_name=\"predict_roberta.yaml\", cat=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tokenizing:   0%|          | 0/676 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676/676 [00:00<00:00, 4369.16it/s]\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/digital_breakthrough/task_3/models/lm_models/roberta_wordpiece_100k/final\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"dim\": 512,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dim\": 2048,\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"roberta\",\n","  \"n_heads\": 8,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 100000\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["3/3 [==============================] - 4s 245ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1GnLnW5xzxOv"},"source":["# tst = (\n","#     last_predict_image[last_predict_image.sign_0 != -1]['sign_0']\n","#     .map(MAPPER)\n","#     .map(label_to_typology)\n","# )\n","sample_submission.loc[pred_desc.index, 'typology'] = pred_desc2.values\n","sample_submission.loc[tst.index, 'typology'] = tst.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWwY0WLm31Ky"},"source":["sample_submission.to_csv(SUB_PATH / 'imroberta2.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9BuRh4F_wTz"},"source":["import tensorflow as tf\n","physical_devices = tf.config.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6f786lv48vLz","executionInfo":{"status":"ok","timestamp":1628097985045,"user_tz":-180,"elapsed":1675979,"user":{"displayName":"Aleksandr Shirokov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsVAQHPkRgO2wPgkH9jP3xC6JRooI58PuUGJHd=s64","userId":"08072078113294094856"}},"outputId":"9b60c3b9-bbd0-4f65-c592-28690aab9b1d"},"source":["train_description_model(config_name='rob_pretrained.yaml', skip_fitting_tokenizer=True, skip_fitting_lm_model=True)"],"execution_count":107,"outputs":[{"output_type":"stream","text":["Savedir: /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5\n","0                                               –≥—Ä–∞—Ñ–∏–∫–∞\n","1                                             –¥–æ–∫—É–º–µ–Ω—Ç—ã\n","2                                              –∂–∏–≤–æ–ø–∏—Å—å\n","3                                                –æ—Ä—É–∂–∏–µ\n","4                                   –ø—Ä–µ–¥–º–µ—Ç—ã –∞—Ä—Ö–µ–æ–ª–æ–≥–∏–∏\n","5                 –ø—Ä–µ–¥–º–µ—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","6                   –ø—Ä–µ–¥–º–µ—Ç—ã –º–∏–Ω–µ—Ä–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n","7                                  –ø—Ä–µ–¥–º–µ—Ç—ã –Ω—É–º–∏–∑–º–∞—Ç–∏–∫–∏\n","8                           –ø—Ä–µ–¥–º–µ—Ç—ã –ø–µ—á–∞—Ç–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏\n","9     –ø—Ä–µ–¥–º–µ—Ç—ã –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞, –±—ã—Ç–∞ –∏ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏–∏\n","10                                     –ø—Ä–µ–¥–º–µ—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏\n","11                                               –ø—Ä–æ—á–∏–µ\n","12                                         —Ä–µ–¥–∫–∏–µ –∫–Ω–∏–≥–∏\n","13                                           —Å–∫—É–ª—å–ø—Ç—É—Ä–∞\n","14                                —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã\n","Name: category, dtype: object\n","/content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/item_name.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/item_name_train.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/item_name_valid.txt /content/drive/MyDrive/digital_breakthrough/task_3/data/description/v5/train_data.csv 15\n"],"name":"stdout"},{"output_type":"stream","text":["loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/7fbac018f5fe478995bca409059883685f3f1325a67b1cb16fe340325af1cea7.018f85b6550237c27386c0ec90a1ff7bdcf74e56a9e2d32131e29c4689192eaa\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/0c7965501fba622370adf334149813060bb231408a6582b155bf623cd1f81d47.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ab4807a64e393da687bd95f59450f96a90d378d830de98889b58383c82a99480.7d8cf5940d60559bc588a922cc36816803bd6aa4db1f2dd48db71df501ac6ea3\n","loading file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/tokenizer.json from cache at None\n","loading configuration file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c6da12aa84b0056f7fcf2ce40343ab08fd71045914c431fd8bf57709efc5abae.e8f15c5aad2f4653e46ceeba0bb32c02a629d106a902c964bce60523d290ac8f\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","tokenizing:   0%|          | 0/157449 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157449/157449 [01:19<00:00, 1983.22it/s]\n","loading configuration file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c6da12aa84b0056f7fcf2ce40343ab08fd71045914c431fd8bf57709efc5abae.e8f15c5aad2f4653e46ceeba0bb32c02a629d106a902c964bce60523d290ac8f\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"directionality\": \"bidi\",\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.9.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file https://huggingface.co/DeepPavlov/rubert-base-cased-sentence/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e652de99ad5f614a638b468be975a6b8af6466fc9d9285e96bab58e6da989384.b0463ea0c0c83c6c37032f7b1168f52dad0807c3c45c67c2995b9714a4397429\n","Loading PyTorch weights from /root/.cache/huggingface/transformers/e652de99ad5f614a638b468be975a6b8af6466fc9d9285e96bab58e6da989384.b0463ea0c0c83c6c37032f7b1168f52dad0807c3c45c67c2995b9714a4397429\n","PyTorch checkpoint contains 177,853,440 parameters\n","Loaded 177,853,440 parameters in the TF 2.0 model.\n","All PyTorch model weights were used when initializing TFRobertaModel.\n","\n","All the weights of TFRobertaModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_6\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_15 (InputLayer)           [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","input_16 (InputLayer)           [(None, 32)]         0                                            \n","__________________________________________________________________________________________________\n","roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 177853440   input_15[0][0]                   \n","                                                                 input_16[0][0]                   \n","__________________________________________________________________________________________________\n","global_average_pooling1d_6 (Glo (None, 768)          0           roberta[0][13]                   \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 15)           11535       global_average_pooling1d_6[0][0] \n","==================================================================================================\n","Total params: 177,864,975\n","Trainable params: 177,864,975\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/5\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["1723/1723 [==============================] - 552s 309ms/step - loss: 0.7615 - f1_score: nan - val_loss: 0.6447 - val_f1_score: 0.7904\n","Epoch 2/5\n","1723/1723 [==============================] - 529s 307ms/step - loss: 0.5461 - f1_score: 0.8225 - val_loss: 0.6135 - val_f1_score: 0.8100\n","Epoch 3/5\n","1723/1723 [==============================] - 530s 307ms/step - loss: 0.4513 - f1_score: 0.8522 - val_loss: 0.6254 - val_f1_score: 0.8140\n","Epoch 4/5\n","1723/1723 [==============================] - 530s 308ms/step - loss: 0.3711 - f1_score: 0.8776 - val_loss: 0.6653 - val_f1_score: 0.8093\n","Epoch 5/5\n","1723/1723 [==============================] - 530s 307ms/step - loss: 0.2972 - f1_score: 0.9017 - val_loss: 0.7177 - val_f1_score: 0.8059\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses, pooler_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 1055). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_pretrained_wordpiece_100k/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/digital_breakthrough/task_3/models/model/roberta_pretrained_wordpiece_100k/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wXev_i6x_G9-"},"source":[""],"execution_count":null,"outputs":[]}]}